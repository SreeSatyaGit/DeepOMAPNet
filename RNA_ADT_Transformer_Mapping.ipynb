{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a2c6132",
   "metadata": {},
   "source": [
    "# Transformer Mapping between GAT Embeddings of RNA and ADT\n",
    "\n",
    "This notebook learns a mapping between GAT embeddings from RNA data and GAT embeddings from ADT data using a Transformer Encoder architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdde7f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory optimization and system check\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Set memory management environment variables\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "# Check system resources\n",
    "print(\"=== System Resources ===\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.cuda.current_device()\n",
    "    gpu_props = torch.cuda.get_device_properties(device)\n",
    "    total_memory = gpu_props.total_memory / (1024**3)  # Convert to GB\n",
    "    \n",
    "    print(f\"GPU: {gpu_props.name}\")\n",
    "    print(f\"Total GPU Memory: {total_memory:.1f} GB\")\n",
    "    print(f\"GPU Compute Capability: {gpu_props.major}.{gpu_props.minor}\")\n",
    "    \n",
    "    # Clear any cached memory\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Check current memory usage\n",
    "    allocated = torch.cuda.memory_allocated(device) / (1024**3)\n",
    "    reserved = torch.cuda.memory_reserved(device) / (1024**3)\n",
    "    \n",
    "    print(f\"Currently allocated: {allocated:.2f} GB\")\n",
    "    print(f\"Currently reserved: {reserved:.2f} GB\")\n",
    "    print(f\"Available: {total_memory - reserved:.2f} GB\")\n",
    "    \n",
    "    # Set recommendations based on available memory\n",
    "    if total_memory < 8:\n",
    "        print(\"\\nâš ï¸  WARNING: Low GPU memory detected!\")\n",
    "        print(\"Recommendations:\")\n",
    "        print(\"- Use CPU fallback if needed\")\n",
    "        print(\"- Reduce batch sizes\")\n",
    "        print(\"- Use graph sparsification\")\n",
    "    elif total_memory < 16:\n",
    "        print(\"\\nðŸ’¡ Moderate GPU memory - will use optimized settings\")\n",
    "    else:\n",
    "        print(\"\\nâœ… Sufficient GPU memory available\")\n",
    "        \n",
    "else:\n",
    "    print(\"CUDA not available - will use CPU\")\n",
    "    print(\"Note: Training will be slower but should work with larger graphs\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24112ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Set environment variables for better memory management\n",
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import pandas as pd\n",
    "\n",
    "import scanpy as sc\n",
    "import scanpy.external as sce\n",
    "from scipy import sparse\n",
    "\n",
    "from DeepOMAPNet.Preprocess import prepare_train_test_anndata\n",
    "\n",
    "# Set memory management\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current GPU: {torch.cuda.current_device()}\")\n",
    "    print(f\"GPU name: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.1f} GB\")\n",
    "else:\n",
    "    print(\"CUDA not available, using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574c2a7a",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b709d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessed data\n",
    "data = prepare_train_test_anndata()\n",
    "trainGene = data[0]  # RNA data\n",
    "trainADT = data[2]   # ADT data\n",
    "\n",
    "print(f\"RNA data shape: {trainGene.shape}\")\n",
    "print(f\"ADT data shape: {trainADT.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e234eb83",
   "metadata": {},
   "source": [
    "## 2. Preprocess RNA Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f546328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNA preprocessing\n",
    "sc.pp.normalize_total(trainGene, target_sum=1e4)\n",
    "sc.pp.log1p(trainGene)\n",
    "sc.pp.highly_variable_genes(trainGene, n_top_genes=2000, batch_key=\"samples\")\n",
    "trainGene = trainGene[:, trainGene.var.highly_variable].copy()\n",
    "\n",
    "sc.pp.scale(trainGene, max_value=10)\n",
    "sc.tl.pca(trainGene, n_comps=50, svd_solver=\"arpack\")\n",
    "\n",
    "# Build neighbor graph for RNA\n",
    "sc.pp.neighbors(trainGene, n_neighbors=15, n_pcs=50)\n",
    "sc.tl.leiden(trainGene, resolution=1.0)\n",
    "\n",
    "print(f\"RNA data after preprocessing: {trainGene.shape}\")\n",
    "print(f\"Number of RNA clusters: {trainGene.obs['leiden'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb07a02",
   "metadata": {},
   "source": [
    "## 3. Preprocess ADT Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df180c9",
   "metadata": {},
   "source": [
    "## 3. Centered Log-Ratio (CLR) Normalization for ADT Data\n",
    "\n",
    "Before we apply standard preprocessing steps, we'll perform Centered Log-Ratio (CLR) normalization on the ADT data. CLR normalization is particularly suited for ADT/CITE-seq data because:\n",
    "\n",
    "1. It handles the compositional nature of the data\n",
    "2. It preserves relative differences between markers\n",
    "3. It reduces technical noise while maintaining biological signal\n",
    "\n",
    "The CLR transformation is defined as:\n",
    "\n",
    "$$\\text{CLR}(x) = \\log(x) - \\frac{1}{D}\\sum_{i=1}^{D}\\log(x_i)$$\n",
    "\n",
    "Where $D$ is the number of features (ADT markers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68aa845",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clr_normalize(adata, axis=1, pseudo_count=1):\n",
    "    \"\"\"\n",
    "    Apply centered log-ratio normalization to the data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    adata : AnnData\n",
    "        AnnData object with raw counts\n",
    "    axis : int, default=1\n",
    "        0 = normalize features (columns), 1 = normalize cells (rows)\n",
    "    pseudo_count : float, default=1\n",
    "        Value to add to counts before log transform to avoid log(0)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    AnnData with CLR-normalized values in .X\n",
    "    \"\"\"\n",
    "    print(\"Applying CLR normalization to ADT data...\")\n",
    "    \n",
    "    # Make a copy to avoid modifying the original\n",
    "    adata_clr = adata.copy()\n",
    "    \n",
    "    # Get raw counts (densify if sparse)\n",
    "    X = adata_clr.X.toarray() if scipy.sparse.issparse(adata_clr.X) else adata_clr.X.copy()\n",
    "    \n",
    "    # Add pseudo count\n",
    "    X += pseudo_count\n",
    "    \n",
    "    # Calculate geometric mean of each cell (row) or feature (column)\n",
    "    if axis == 1:  # across features (for each cell)\n",
    "        # Get geometric mean for each cell\n",
    "        geometric_means = np.exp(np.mean(np.log(X), axis=1, keepdims=True))\n",
    "        # CLR transformation\n",
    "        X_clr = np.log(X / geometric_means)\n",
    "    else:  # across cells (for each feature)\n",
    "        # Get geometric mean for each feature\n",
    "        geometric_means = np.exp(np.mean(np.log(X), axis=0, keepdims=True))\n",
    "        # CLR transformation\n",
    "        X_clr = np.log(X / geometric_means)\n",
    "    \n",
    "    # Update data\n",
    "    adata_clr.X = X_clr\n",
    "    \n",
    "    # Store original data in raw slot\n",
    "    adata_clr.raw = adata\n",
    "    \n",
    "    print(f\"CLR normalization complete. Shape: {adata_clr.X.shape}\")\n",
    "    return adata_clr\n",
    "\n",
    "# Apply CLR normalization to ADT data\n",
    "trainADT_clr = clr_normalize(trainADT)\n",
    "\n",
    "# Basic quality check - visualize distribution before and after normalization\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Original data distribution\n",
    "if scipy.sparse.issparse(trainADT.X):\n",
    "    sample_values = trainADT.X.data[:10000] if len(trainADT.X.data) > 10000 else trainADT.X.data\n",
    "else:\n",
    "    sample_values = trainADT.X.flatten()[:10000]\n",
    "    \n",
    "sns.histplot(sample_values, bins=50, kde=True, ax=ax[0])\n",
    "ax[0].set_title(\"Original ADT Values\")\n",
    "ax[0].set_xlabel(\"Value\")\n",
    "\n",
    "# CLR-normalized data distribution\n",
    "sample_values_clr = trainADT_clr.X.flatten()[:10000]\n",
    "sns.histplot(sample_values_clr, bins=50, kde=True, ax=ax[1])\n",
    "ax[1].set_title(\"CLR-Normalized ADT Values\")\n",
    "ax[1].set_xlabel(\"Value\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Replace original ADT data with CLR-normalized data for further processing\n",
    "trainADT = trainADT_clr\n",
    "\n",
    "print(\"ADT data now uses CLR normalization\")\n",
    "print(f\"ADT data shape: {trainADT.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60446102",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4. Additional ADT Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47920cdd",
   "metadata": {},
   "source": [
    "## 5. Build PyTorch Geometric Data Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca6654f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparsify_graph(adata, max_edges_per_node=50):\n",
    "    \"\"\"Sparsify the graph by keeping only top k neighbors per node\"\"\"\n",
    "    \n",
    "    A = adata.obsp[\"connectivities\"].tocsr()\n",
    "    n_nodes = A.shape[0]\n",
    "    \n",
    "    # Check if sparsification is needed\n",
    "    avg_degree = A.nnz / n_nodes\n",
    "    if avg_degree <= max_edges_per_node:\n",
    "        print(f\"Graph already sparse enough (avg degree: {avg_degree:.1f})\")\n",
    "        return adata\n",
    "    \n",
    "    print(f\"Sparsifying graph from avg degree {avg_degree:.1f} to max {max_edges_per_node}\")\n",
    "    \n",
    "    # Create new sparse matrix\n",
    "    row_indices = []\n",
    "    col_indices = []\n",
    "    data_values = []\n",
    "    \n",
    "    for i in range(n_nodes):\n",
    "        # Get neighbors and their weights for node i\n",
    "        start_idx = A.indptr[i]\n",
    "        end_idx = A.indptr[i + 1]\n",
    "        neighbors = A.indices[start_idx:end_idx]\n",
    "        weights = A.data[start_idx:end_idx]\n",
    "        \n",
    "        # Keep only top k neighbors\n",
    "        if len(neighbors) > max_edges_per_node:\n",
    "            top_k_indices = np.argpartition(weights, -max_edges_per_node)[-max_edges_per_node:]\n",
    "            neighbors = neighbors[top_k_indices]\n",
    "            weights = weights[top_k_indices]\n",
    "        \n",
    "        # Add edges\n",
    "        row_indices.extend([i] * len(neighbors))\n",
    "        col_indices.extend(neighbors)\n",
    "        data_values.extend(weights)\n",
    "    \n",
    "    # Create new adjacency matrix\n",
    "    A_sparse = sparse.csr_matrix(\n",
    "        (data_values, (row_indices, col_indices)), \n",
    "        shape=(n_nodes, n_nodes)\n",
    "    )\n",
    "    \n",
    "    # Make symmetric\n",
    "    A_sparse = (A_sparse + A_sparse.T) / 2\n",
    "    \n",
    "    # Update the AnnData object\n",
    "    adata.obsp[\"connectivities\"] = A_sparse\n",
    "    \n",
    "    new_avg_degree = A_sparse.nnz / n_nodes\n",
    "    print(f\"New average degree: {new_avg_degree:.1f}\")\n",
    "    \n",
    "    return adata\n",
    "\n",
    "def build_pyg_data(adata, use_pca=True, sparsify_large_graphs=True, max_edges_per_node=50):\n",
    "    \"\"\"Build PyTorch Geometric Data object from AnnData\"\"\"\n",
    "    \n",
    "    # Sparsify if needed\n",
    "    if sparsify_large_graphs:\n",
    "        A = adata.obsp[\"connectivities\"]\n",
    "        avg_degree = A.nnz / A.shape[0]\n",
    "        if avg_degree > max_edges_per_node:\n",
    "            print(f\"Large graph detected (avg degree: {avg_degree:.1f}), applying sparsification...\")\n",
    "            adata = sparsify_graph(adata, max_edges_per_node)\n",
    "    \n",
    "    # Features\n",
    "    X = adata.obsm[\"X_pca\"] if use_pca else adata.X.toarray()\n",
    "    \n",
    "    # Labels (leiden clusters)\n",
    "    y = adata.obs[\"leiden\"].astype(int).to_numpy()\n",
    "    \n",
    "    # Edge index from connectivities\n",
    "    A = adata.obsp[\"connectivities\"].tocsr()\n",
    "    A_triu = sparse.triu(A, k=1)\n",
    "    row, col = A_triu.nonzero()\n",
    "    edge_index = torch.tensor(np.vstack([row, col]), dtype=torch.long)\n",
    "    \n",
    "    # Create PyG Data object\n",
    "    data = Data(\n",
    "        x=torch.tensor(X, dtype=torch.float32),\n",
    "        edge_index=edge_index,\n",
    "        y=torch.tensor(y, dtype=torch.long),\n",
    "    )\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Build data objects with memory optimization\n",
    "print(\"Building PyG data objects...\")\n",
    "\n",
    "# Check available GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # GB\n",
    "    print(f\"Available GPU memory: {gpu_memory:.1f} GB\")\n",
    "    \n",
    "    # Estimate memory requirements\n",
    "    rna_edges = trainGene.obsp[\"connectivities\"].nnz\n",
    "    adt_edges = trainADT.obsp[\"connectivities\"].nnz\n",
    "    \n",
    "    print(f\"RNA graph edges: {rna_edges:,}\")\n",
    "    print(f\"ADT graph edges: {adt_edges:,}\")\n",
    "    \n",
    "    # Set sparsification based on graph size\n",
    "    max_edges_rna = 100 if rna_edges > 5000000 else 200\n",
    "    max_edges_adt = 50 if adt_edges > 10000000 else 100\n",
    "    \n",
    "    print(f\"Using max edges per node - RNA: {max_edges_rna}, ADT: {max_edges_adt}\")\n",
    "else:\n",
    "    print(\"Using CPU - no memory constraints\")\n",
    "    max_edges_rna = 200\n",
    "    max_edges_adt = 100\n",
    "\n",
    "# Build data objects\n",
    "rna_data = build_pyg_data(trainGene, use_pca=True, sparsify_large_graphs=True, max_edges_per_node=max_edges_rna)\n",
    "adt_data = build_pyg_data(trainADT, use_pca=True, sparsify_large_graphs=True, max_edges_per_node=max_edges_adt)\n",
    "\n",
    "print(f\"RNA PyG data - Nodes: {rna_data.num_nodes}, Edges: {rna_data.num_edges}, Features: {rna_data.num_node_features}\")\n",
    "print(f\"ADT PyG data - Nodes: {adt_data.num_nodes}, Edges: {adt_data.num_edges}, Features: {adt_data.num_node_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43647fa",
   "metadata": {},
   "source": [
    "## 6. Define GAT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cc5c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGAT(torch.nn.Module):\n",
    "    \"\"\"Simplified GAT for memory-constrained scenarios\"\"\"\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, heads=4, dropout=0.6):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # Single GAT layer for memory efficiency\n",
    "        self.conv1 = GATConv(in_channels, out_channels, heads=heads, dropout=dropout, concat=False)\n",
    "        \n",
    "    def forward(self, x, edge_index, return_embeddings=False):\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.conv1(x, edge_index)\n",
    "        \n",
    "        if return_embeddings:\n",
    "            return x\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def get_embeddings(self, x, edge_index):\n",
    "        \"\"\"Get embeddings for mapping\"\"\"\n",
    "        return self.forward(x, edge_index, return_embeddings=True)\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, heads=8, dropout=0.6):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.conv1 = GATConv(in_channels, hidden_channels, heads=heads, dropout=dropout)\n",
    "        self.conv2 = GATConv(hidden_channels * heads, out_channels, heads=1, dropout=dropout)\n",
    "        \n",
    "    def forward(self, x, edge_index, return_embeddings=False):\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        \n",
    "        if return_embeddings:\n",
    "            return x  # Return embeddings before final layer\n",
    "            \n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "    def get_embeddings(self, x, edge_index):\n",
    "        \"\"\"Get intermediate embeddings for mapping\"\"\"\n",
    "        return self.forward(x, edge_index, return_embeddings=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe2098d",
   "metadata": {},
   "source": [
    "## 7. Train GAT Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd0f7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gat_model(data, model_name=\"GAT\", epochs=200, use_cpu_fallback=False):\n",
    "    \"\"\"Train a GAT model and return the trained model\"\"\"\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() and not use_cpu_fallback else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Check memory requirements and adjust accordingly\n",
    "    num_edges = data.num_edges\n",
    "    num_nodes = data.num_nodes\n",
    "    \n",
    "    print(f\"Graph stats - Nodes: {num_nodes}, Edges: {num_edges}\")\n",
    "    \n",
    "    # Memory optimization: reduce model size if too many edges\n",
    "    use_simple_model = False\n",
    "    if num_edges > 2000000:  # If more than 2M edges\n",
    "        print(\"Very large graph detected, using simplified GAT architecture...\")\n",
    "        hidden_dim = 32\n",
    "        heads = 4\n",
    "        use_simple_model = True\n",
    "    elif num_edges > 1000000:  # If more than 1M edges\n",
    "        print(\"Large graph detected, reducing model complexity...\")\n",
    "        hidden_dim = 32\n",
    "        heads = 4\n",
    "    else:\n",
    "        hidden_dim = 64\n",
    "        heads = 8\n",
    "    \n",
    "    # Create train/val/test masks\n",
    "    N = data.num_nodes\n",
    "    y_np = data.y.cpu().numpy()\n",
    "    \n",
    "    from sklearn.model_selection import StratifiedShuffleSplit\n",
    "    \n",
    "    # Split 80/10/10\n",
    "    sss1 = StratifiedShuffleSplit(n_splits=1, train_size=0.8, random_state=42)\n",
    "    train_idx, temp_idx = next(sss1.split(np.zeros(N), y_np))\n",
    "    \n",
    "    y_temp = y_np[temp_idx]\n",
    "    sss2 = StratifiedShuffleSplit(n_splits=1, train_size=0.5, random_state=43)\n",
    "    val_rel, test_rel = next(sss2.split(np.zeros(len(temp_idx)), y_temp))\n",
    "    val_idx = temp_idx[val_rel]\n",
    "    test_idx = temp_idx[test_rel]\n",
    "    \n",
    "    train_mask = torch.zeros(N, dtype=torch.bool)\n",
    "    val_mask = torch.zeros(N, dtype=torch.bool)\n",
    "    test_mask = torch.zeros(N, dtype=torch.bool)\n",
    "    train_mask[train_idx] = True\n",
    "    val_mask[val_idx] = True\n",
    "    test_mask[test_idx] = True\n",
    "    \n",
    "    data.train_mask = train_mask\n",
    "    data.val_mask = val_mask\n",
    "    data.test_mask = test_mask\n",
    "    \n",
    "    # Initialize model\n",
    "    in_dim = data.x.size(1)\n",
    "    n_class = int(data.y.max().item() + 1)\n",
    "    \n",
    "    if use_simple_model:\n",
    "        model = SimpleGAT(in_dim, hidden_dim, n_class, heads=heads).to(device)\n",
    "        print(f\"Using SimpleGAT: {in_dim} -> {n_class} (hidden: {hidden_dim}, heads: {heads})\")\n",
    "    else:\n",
    "        model = GAT(in_dim, hidden_dim, n_class, heads=heads).to(device)\n",
    "        print(f\"Using GAT: {in_dim} -> {hidden_dim} -> {n_class} (heads: {heads})\")\n",
    "    \n",
    "    # Move data to device with memory management\n",
    "    cpu_fallback_triggered = False\n",
    "    try:\n",
    "        data = data.to(device)\n",
    "        print(f\"Successfully moved data to {device}\")\n",
    "    except RuntimeError as e:\n",
    "        if \"out of memory\" in str(e).lower():\n",
    "            print(f\"GPU memory insufficient, falling back to CPU...\")\n",
    "            device = torch.device('cpu')\n",
    "            model = model.cpu()\n",
    "            data = data.cpu()\n",
    "            cpu_fallback_triggered = True\n",
    "        else:\n",
    "            raise e\n",
    "    \n",
    "    # Initialize optimizer and criterion\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    def train():\n",
    "        nonlocal model, data, optimizer, device, cpu_fallback_triggered\n",
    "        \n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        try:\n",
    "            if device.type == 'cuda':\n",
    "                torch.cuda.empty_cache()  # Clear cache before forward pass\n",
    "            \n",
    "            out = model(data.x, data.edge_index)\n",
    "            loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if device.type == 'cuda':\n",
    "                torch.cuda.empty_cache()  # Clear cache after backward pass\n",
    "                \n",
    "            return loss\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e).lower() and not cpu_fallback_triggered:\n",
    "                print(f\"GPU OOM during training, switching to CPU...\")\n",
    "                # Move everything to CPU\n",
    "                device = torch.device('cpu')\n",
    "                model = model.cpu()\n",
    "                data = data.cpu()\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "                cpu_fallback_triggered = True\n",
    "                \n",
    "                # Retry the forward pass on CPU\n",
    "                optimizer.zero_grad()\n",
    "                out = model(data.x, data.edge_index)\n",
    "                loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                return loss\n",
    "            else:\n",
    "                raise e\n",
    "    \n",
    "    def test(mask):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            if device.type == 'cuda':\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "            out = model(data.x, data.edge_index)\n",
    "            pred = out.argmax(dim=1)\n",
    "            correct = pred[mask] == data.y[mask]\n",
    "            acc = int(correct.sum()) / int(mask.sum())\n",
    "            return acc\n",
    "    \n",
    "    print(f\"Training {model_name} model...\")\n",
    "    best_val_acc = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        loss = train()\n",
    "        \n",
    "        if epoch % 50 == 0 or epoch == 1:\n",
    "            val_acc = test(data.val_mask)\n",
    "            test_acc = test(data.test_mask)\n",
    "            print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_acc:.4f}, Test: {test_acc:.4f}')\n",
    "            \n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                best_model_state = model.state_dict().copy()\n",
    "    \n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    final_test_acc = test(data.test_mask)\n",
    "    print(f\"Final {model_name} test accuracy: {final_test_acc:.4f}\")\n",
    "    \n",
    "    return model, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f51e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train GAT models for both RNA and ADT with memory management\n",
    "print(\"=== Training RNA GAT ===\")\n",
    "try:\n",
    "    rna_gat_model, rna_data_with_masks = train_gat_model(rna_data, \"RNA GAT\", epochs=200)\n",
    "except RuntimeError as e:\n",
    "    if \"out of memory\" in str(e).lower():\n",
    "        print(\"GPU memory insufficient for RNA GAT, trying CPU...\")\n",
    "        rna_gat_model, rna_data_with_masks = train_gat_model(rna_data, \"RNA GAT\", epochs=200, use_cpu_fallback=True)\n",
    "    else:\n",
    "        raise e\n",
    "\n",
    "print(\"\\n=== Training ADT GAT ===\")\n",
    "try:\n",
    "    adt_gat_model, adt_data_with_masks = train_gat_model(adt_data, \"ADT GAT\", epochs=200)\n",
    "except RuntimeError as e:\n",
    "    if \"out of memory\" in str(e).lower():\n",
    "        print(\"GPU memory insufficient for ADT GAT, trying CPU...\")\n",
    "        adt_gat_model, adt_data_with_masks = train_gat_model(adt_data, \"ADT GAT\", epochs=200, use_cpu_fallback=True)\n",
    "    else:\n",
    "        raise e\n",
    "\n",
    "print(\"\\n=== GAT Training Complete ===\")\n",
    "print(f\"RNA GAT model trained successfully\")\n",
    "print(f\"ADT GAT model trained successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a896646",
   "metadata": {},
   "source": [
    "## 8. Extract GAT Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5c8cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings(model, data):\n",
    "    \"\"\"Extract embeddings from trained GAT model\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Ensure model and data are on the same device\n",
    "    device = next(model.parameters()).device\n",
    "    if data.x.device != device:\n",
    "        print(f\"Moving data from {data.x.device} to {device}\")\n",
    "        data = data.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Clear cache if using GPU\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        embeddings = model.get_embeddings(data.x, data.edge_index)\n",
    "        \n",
    "        # Move to CPU for further processing\n",
    "        embeddings = embeddings.cpu()\n",
    "        \n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "    return embeddings\n",
    "\n",
    "# Clear any existing cache\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Extract embeddings\n",
    "print(\"Extracting embeddings...\")\n",
    "rna_embeddings = extract_embeddings(rna_gat_model, rna_data_with_masks)\n",
    "adt_embeddings = extract_embeddings(adt_gat_model, adt_data_with_masks)\n",
    "\n",
    "print(f\"RNA embeddings shape: {rna_embeddings.shape}\")\n",
    "print(f\"ADT embeddings shape: {adt_embeddings.shape}\")\n",
    "\n",
    "# Ensure both embeddings have the same number of cells\n",
    "assert rna_embeddings.shape[0] == adt_embeddings.shape[0], \"Number of cells must match\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1635b84",
   "metadata": {},
   "source": [
    "## 9. Define Transformer Encoder Mapping Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247dc1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Positional encoding for transformer models\"\"\"\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerMapping(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, d_model=256, nhead=4, num_layers=3, dropout=0.1):\n",
    "        super(TransformerMapping, self).__init__()\n",
    "        \n",
    "        # Input projection\n",
    "        self.input_proj = nn.Linear(input_dim, d_model)\n",
    "        \n",
    "        # Transformer encoder\n",
    "        encoder_layers = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=d_model*4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_proj = nn.Linear(d_model, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Project input to transformer dimensions\n",
    "        x = self.input_proj(x)\n",
    "        \n",
    "        # Add batch dimension if not present\n",
    "        if len(x.shape) == 2:\n",
    "            x = x.unsqueeze(1)  # [batch_size, 1, d_model]\n",
    "            \n",
    "        # Pass through transformer encoder\n",
    "        x = self.transformer_encoder(x)\n",
    "        \n",
    "        # Project to output dimensions\n",
    "        x = self.output_proj(x.squeeze(1))\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Initialize transformer mapping model\n",
    "import math  # For the positional encoding\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "input_dim = rna_embeddings.shape[1]\n",
    "output_dim = adt_embeddings.shape[1]\n",
    "\n",
    "transformer_model = TransformerMapping(\n",
    "    input_dim=input_dim, \n",
    "    output_dim=output_dim, \n",
    "    d_model=256,\n",
    "    nhead=4,\n",
    "    num_layers=3\n",
    ").to(device)\n",
    "\n",
    "print(f\"Transformer Model: {input_dim} -> {output_dim}\")\n",
    "print(transformer_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959f2856",
   "metadata": {},
   "source": [
    "## 9. Prepare Training Data for Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee30a7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert embeddings to CPU and numpy\n",
    "rna_emb_np = rna_embeddings.cpu().numpy()\n",
    "adt_emb_np = adt_embeddings.cpu().numpy()\n",
    "\n",
    "# Split data for training (use same train/val/test split as GAT)\n",
    "train_mask_np = rna_data_with_masks.train_mask.cpu().numpy()\n",
    "val_mask_np = rna_data_with_masks.val_mask.cpu().numpy()\n",
    "test_mask_np = rna_data_with_masks.test_mask.cpu().numpy()\n",
    "\n",
    "# Prepare training data\n",
    "X_train = torch.tensor(rna_emb_np[train_mask_np], dtype=torch.float32)\n",
    "y_train = torch.tensor(adt_emb_np[train_mask_np], dtype=torch.float32)\n",
    "\n",
    "X_val = torch.tensor(rna_emb_np[val_mask_np], dtype=torch.float32)\n",
    "y_val = torch.tensor(adt_emb_np[val_mask_np], dtype=torch.float32)\n",
    "\n",
    "X_test = torch.tensor(rna_emb_np[test_mask_np], dtype=torch.float32)\n",
    "y_test = torch.tensor(adt_emb_np[test_mask_np], dtype=torch.float32)\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 64  # Smaller batch size for transformer\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb95d0c8",
   "metadata": {},
   "source": [
    "## 10. Train Transformer Mapping Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2171a932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "num_epochs = 300\n",
    "learning_rate = 0.0005  # Lower learning rate for transformer\n",
    "weight_decay = 1e-4\n",
    "\n",
    "# Warmup scheduler\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "def get_lr_scheduler(optimizer, warmup_steps=1000, max_steps=10000):\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < warmup_steps:\n",
    "            return float(current_step) / float(max(1, warmup_steps))\n",
    "        return max(\n",
    "            0.0, float(max_steps - current_step) / float(max(1, max_steps - warmup_steps))\n",
    "        )\n",
    "    return LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(transformer_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)\n",
    "\n",
    "# Training loop\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "best_model_state = None\n",
    "patience = 30\n",
    "patience_counter = 0\n",
    "\n",
    "print(\"Training Transformer mapping model...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    transformer_model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for batch_x, batch_y in train_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = transformer_model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(transformer_model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Validation phase\n",
    "    transformer_model.eval()\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in val_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            outputs = transformer_model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    val_loss /= len(val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = transformer_model.state_dict().copy()\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    if (epoch + 1) % 25 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}')\n",
    "    \n",
    "    if patience_counter >= patience:\n",
    "        print(f'Early stopping at epoch {epoch+1}')\n",
    "        break\n",
    "\n",
    "# Load best model\n",
    "transformer_model.load_state_dict(best_model_state)\n",
    "print(f'Best validation loss: {best_val_loss:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f004149a",
   "metadata": {},
   "source": [
    "## 11. Evaluate Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ceb5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "transformer_model.eval()\n",
    "test_loss = 0.0\n",
    "predictions = []\n",
    "ground_truth = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y in test_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        outputs = transformer_model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        predictions.append(outputs.cpu().numpy())\n",
    "        ground_truth.append(batch_y.cpu().numpy())\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "predictions = np.vstack(predictions)\n",
    "ground_truth = np.vstack(ground_truth)\n",
    "\n",
    "# Calculate metrics\n",
    "mse = mean_squared_error(ground_truth, predictions)\n",
    "r2 = r2_score(ground_truth, predictions)\n",
    "\n",
    "# Calculate correlation per dimension\n",
    "pearson_corrs = []\n",
    "spearman_corrs = []\n",
    "\n",
    "for i in range(ground_truth.shape[1]):\n",
    "    pearson_r, _ = pearsonr(ground_truth[:, i], predictions[:, i])\n",
    "    spearman_r, _ = spearmanr(ground_truth[:, i], predictions[:, i])\n",
    "    pearson_corrs.append(pearson_r)\n",
    "    spearman_corrs.append(spearman_r)\n",
    "\n",
    "mean_pearson = np.mean(pearson_corrs)\n",
    "mean_spearman = np.mean(spearman_corrs)\n",
    "\n",
    "print(f\"\\n=== Transformer Mapping Results ===\")\n",
    "print(f\"Test Loss (MSE): {test_loss:.6f}\")\n",
    "print(f\"MSE: {mse:.6f}\")\n",
    "print(f\"RÂ² Score: {r2:.4f}\")\n",
    "print(f\"Mean Pearson Correlation: {mean_pearson:.4f}\")\n",
    "print(f\"Mean Spearman Correlation: {mean_spearman:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f89e08",
   "metadata": {},
   "source": [
    "## 12. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd1612d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(pearson_corrs, bins=20, alpha=0.7, label='Pearson')\n",
    "plt.hist(spearman_corrs, bins=20, alpha=0.7, label='Spearman')\n",
    "plt.xlabel('Correlation')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Per-dimension Correlation Distribution')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8f91f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plots for first few dimensions\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(min(6, ground_truth.shape[1])):\n",
    "    ax = axes[i]\n",
    "    ax.scatter(ground_truth[:, i], predictions[:, i], alpha=0.6, s=1)\n",
    "    \n",
    "    # Add perfect prediction line\n",
    "    min_val = min(ground_truth[:, i].min(), predictions[:, i].min())\n",
    "    max_val = max(ground_truth[:, i].max(), predictions[:, i].max())\n",
    "    ax.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel(f'True ADT Embedding Dim {i+1}')\n",
    "    ax.set_ylabel(f'Predicted ADT Embedding Dim {i+1}')\n",
    "    ax.set_title(f'Dim {i+1}: r={pearson_corrs[i]:.3f}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28abab3",
   "metadata": {},
   "source": [
    "## 13. Compare with MLP Model (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3117e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load MLP model results if available\n",
    "try:\n",
    "    mlp_results = np.load('mapping_predictions.npz')\n",
    "    mlp_predictions = mlp_results['predictions']\n",
    "    mlp_ground_truth = mlp_results['ground_truth']\n",
    "    mlp_pearson = mlp_results['pearson_corrs']\n",
    "    mlp_spearman = mlp_results['spearman_corrs']\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mlp_mse = mean_squared_error(mlp_ground_truth, mlp_predictions)\n",
    "    mlp_r2 = r2_score(mlp_ground_truth, mlp_predictions)\n",
    "    mlp_mean_pearson = np.mean(mlp_pearson)\n",
    "    mlp_mean_spearman = np.mean(mlp_spearman)\n",
    "    \n",
    "    # Compare results\n",
    "    print(\"\\n=== Transformer vs MLP Comparison ===\")\n",
    "    print(f\"MSE: Transformer: {mse:.6f}, MLP: {mlp_mse:.6f}, Improvement: {(mlp_mse-mse)/mlp_mse*100:.2f}%\")\n",
    "    print(f\"RÂ²: Transformer: {r2:.4f}, MLP: {mlp_r2:.4f}, Improvement: {(r2-mlp_r2)/mlp_r2*100:.2f}%\")\n",
    "    print(f\"Pearson: Transformer: {mean_pearson:.4f}, MLP: {mlp_mean_pearson:.4f}, Improvement: {(mean_pearson-mlp_mean_pearson)/mlp_mean_pearson*100:.2f}%\")\n",
    "    print(f\"Spearman: Transformer: {mean_spearman:.4f}, MLP: {mlp_mean_spearman:.4f}, Improvement: {(mean_spearman-mlp_mean_spearman)/mlp_mean_spearman*100:.2f}%\")\n",
    "    \n",
    "    # Visualize correlation distribution comparison\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(pearson_corrs, bins=20, alpha=0.7, label='Transformer')\n",
    "    plt.hist(mlp_pearson, bins=20, alpha=0.7, label='MLP')\n",
    "    plt.xlabel('Pearson Correlation')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Pearson Correlation Comparison')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(spearman_corrs, bins=20, alpha=0.7, label='Transformer')\n",
    "    plt.hist(mlp_spearman, bins=20, alpha=0.7, label='MLP')\n",
    "    plt.xlabel('Spearman Correlation')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Spearman Correlation Comparison')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"MLP results file not found. Cannot compare models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d1875d",
   "metadata": {},
   "source": [
    "## 14. Save Models and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdb92b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained models\n",
    "torch.save({\n",
    "    'rna_gat_state_dict': rna_gat_model.state_dict(),\n",
    "    'adt_gat_state_dict': adt_gat_model.state_dict(),\n",
    "    'transformer_mapping_state_dict': transformer_model.state_dict(),\n",
    "    'rna_input_dim': input_dim,\n",
    "    'adt_output_dim': output_dim,\n",
    "    'test_results': {\n",
    "        'mse': mse,\n",
    "        'r2': r2,\n",
    "        'mean_pearson': mean_pearson,\n",
    "        'mean_spearman': mean_spearman,\n",
    "        'pearson_corrs': pearson_corrs,\n",
    "        'spearman_corrs': spearman_corrs\n",
    "    }\n",
    "}, 'rna_adt_transformer_mapping_models.pth')\n",
    "\n",
    "print(\"Models and results saved to 'rna_adt_transformer_mapping_models.pth'\")\n",
    "\n",
    "# Save predictions for further analysis\n",
    "np.savez('transformer_mapping_predictions.npz', \n",
    "         predictions=predictions, \n",
    "         ground_truth=ground_truth,\n",
    "         pearson_corrs=pearson_corrs,\n",
    "         spearman_corrs=spearman_corrs)\n",
    "\n",
    "print(\"Predictions saved to 'transformer_mapping_predictions.npz'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeaf2327",
   "metadata": {},
   "source": [
    "## 15. Evaluate Cluster Preservation\n",
    "\n",
    "To further evaluate the quality of the transformer encoder mapping, we can check if the predicted ADT embeddings preserve the cluster structure of the original ADT data. We'll apply Leiden clustering to both the true ADT embeddings and predicted ADT embeddings, then measure the agreement between these cluster assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbf0ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create AnnData objects for true and predicted ADT embeddings\n",
    "import anndata as ad\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "import scanpy as sc\n",
    "\n",
    "print(\"Evaluating cluster preservation in predicted embeddings...\")\n",
    "\n",
    "# Create AnnData objects\n",
    "true_adt_adata = ad.AnnData(X=ground_truth)\n",
    "pred_adt_adata = ad.AnnData(X=predictions)\n",
    "\n",
    "# Process both datasets the same way\n",
    "for adata in [true_adt_adata, pred_adt_adata]:\n",
    "    sc.pp.neighbors(adata, n_neighbors=15, use_rep='X')\n",
    "    \n",
    "# Run Leiden clustering with multiple resolutions\n",
    "resolutions = [0.2, 0.5, 0.8, 1.0, 1.5, 2.0]\n",
    "results = []\n",
    "\n",
    "for res in resolutions:\n",
    "    # Cluster true ADT embeddings\n",
    "    sc.tl.leiden(true_adt_adata, resolution=res, key_added=f'leiden_res{res}')\n",
    "    \n",
    "    # Cluster predicted ADT embeddings\n",
    "    sc.tl.leiden(pred_adt_adata, resolution=res, key_added=f'leiden_res{res}')\n",
    "    \n",
    "    # Get cluster labels\n",
    "    true_labels = true_adt_adata.obs[f'leiden_res{res}'].astype(int).values\n",
    "    pred_labels = pred_adt_adata.obs[f'leiden_res{res}'].astype(int).values\n",
    "    \n",
    "    # Calculate metrics\n",
    "    ari = adjusted_rand_score(true_labels, pred_labels)\n",
    "    nmi = normalized_mutual_info_score(true_labels, pred_labels, average_method='arithmetic')\n",
    "    \n",
    "    # Number of clusters\n",
    "    true_n_clusters = len(np.unique(true_labels))\n",
    "    pred_n_clusters = len(np.unique(pred_labels))\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        'Resolution': res,\n",
    "        'True Clusters': true_n_clusters,\n",
    "        'Predicted Clusters': pred_n_clusters,\n",
    "        'ARI': ari,\n",
    "        'NMI': nmi\n",
    "    })\n",
    "\n",
    "# Create results dataframe\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "# Visualize results\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(results_df['Resolution'], results_df['ARI'], 'o-', label='ARI')\n",
    "plt.plot(results_df['Resolution'], results_df['NMI'], 'o-', label='NMI')\n",
    "plt.xlabel('Leiden Resolution')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Clustering Agreement Metrics')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "bar_width = 0.35\n",
    "x = np.arange(len(resolutions))\n",
    "plt.bar(x - bar_width/2, results_df['True Clusters'], bar_width, label='True ADT')\n",
    "plt.bar(x + bar_width/2, results_df['Predicted Clusters'], bar_width, label='Predicted ADT')\n",
    "plt.xlabel('Leiden Resolution')\n",
    "plt.ylabel('Number of Clusters')\n",
    "plt.title('Cluster Counts')\n",
    "plt.xticks(x, resolutions)\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Use resolution with best ARI score for UMAP visualization\n",
    "best_res_idx = results_df['ARI'].idxmax()\n",
    "best_res = results_df.loc[best_res_idx, 'Resolution']\n",
    "print(f\"\\nBest resolution: {best_res} (ARI: {results_df.loc[best_res_idx, 'ARI']:.4f})\")\n",
    "\n",
    "# UMAP visualization of both embeddings with cluster labels\n",
    "for adata in [true_adt_adata, pred_adt_adata]:\n",
    "    sc.tl.umap(adata)\n",
    "\n",
    "# Create a figure for UMAP visualization\n",
    "plt.figure(figsize=(16, 7))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sc.pl.umap(true_adt_adata, color=f'leiden_res{best_res}', title='True ADT Embeddings', show=False, legend_loc='on data')\n",
    "plt.axis('on')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sc.pl.umap(pred_adt_adata, color=f'leiden_res{best_res}', title='Predicted ADT Embeddings', show=False, legend_loc='on data')\n",
    "plt.axis('on')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b0479d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a confusion matrix to see how well clusters match between true and predicted embeddings\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Get cluster labels at the best resolution\n",
    "best_res = results_df.loc[results_df['ARI'].idxmax(), 'Resolution']\n",
    "true_labels = true_adt_adata.obs[f'leiden_res{best_res}'].astype(int)\n",
    "pred_labels = pred_adt_adata.obs[f'leiden_res{best_res}'].astype(int)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels, pred_labels, normalize='true')\n",
    "\n",
    "# Get the number of clusters for proper visualization\n",
    "n_clusters_true = len(np.unique(true_labels))\n",
    "n_clusters_pred = len(np.unique(pred_labels))\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(conf_matrix, cmap=\"YlGnBu\", annot=True if n_clusters_true <= 20 else False,\n",
    "            fmt='.2f', xticklabels=range(n_clusters_pred), yticklabels=range(n_clusters_true))\n",
    "plt.xlabel('Predicted Clusters')\n",
    "plt.ylabel('True Clusters')\n",
    "plt.title(f'Confusion Matrix (Normalized) - Resolution {best_res}')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find the most preserved and least preserved clusters\n",
    "cluster_preservation = np.max(conf_matrix, axis=1)\n",
    "most_preserved_idx = np.argmax(cluster_preservation)\n",
    "least_preserved_idx = np.argmin(cluster_preservation)\n",
    "\n",
    "print(f\"Most preserved cluster: {most_preserved_idx} with {cluster_preservation[most_preserved_idx]:.2%} preservation\")\n",
    "print(f\"Least preserved cluster: {least_preserved_idx} with {cluster_preservation[least_preserved_idx]:.2%} preservation\")\n",
    "\n",
    "# Add cluster labels to the original data for further analysis\n",
    "adata_combined = ad.AnnData(\n",
    "    X=np.concatenate([ground_truth, predictions]),\n",
    "    obs=pd.DataFrame({\n",
    "        'embedding_type': ['True ADT'] * ground_truth.shape[0] + ['Predicted ADT'] * predictions.shape[0],\n",
    "    })\n",
    ")\n",
    "\n",
    "# Calculate UMAP for the combined embedding space\n",
    "sc.pp.neighbors(adata_combined, n_neighbors=15, use_rep='X')\n",
    "sc.tl.umap(adata_combined)\n",
    "\n",
    "# Plot combined UMAP\n",
    "plt.figure(figsize=(12, 10))\n",
    "sc.pl.umap(adata_combined, color='embedding_type', title='Combined UMAP - True vs Predicted ADT Embeddings',\n",
    "           palette={'True ADT': 'blue', 'Predicted ADT': 'red'}, alpha=0.7, size=30, show=False)\n",
    "plt.legend(loc='upper right', frameon=True)\n",
    "plt.show()\n",
    "\n",
    "# Quantify global structure preservation\n",
    "from scipy.spatial import procrustes\n",
    "\n",
    "# Perform Procrustes analysis on UMAP coordinates\n",
    "true_coords = true_adt_adata.obsm['X_umap']\n",
    "pred_coords = pred_adt_adata.obsm['X_umap']\n",
    "\n",
    "# Procrustes analysis scales, rotates and translates the predicted coordinates to best match the true coordinates\n",
    "mtx1, mtx2, disparity = procrustes(true_coords, pred_coords)\n",
    "\n",
    "print(f\"\\nProcrustes analysis disparity (lower is better): {disparity:.4f}\")\n",
    "print(\"This value quantifies how well the global structure is preserved after optimal alignment.\")\n",
    "\n",
    "# Calculate silhouette scores to measure cluster quality\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "try:\n",
    "    true_silhouette = silhouette_score(true_adt_adata.X, true_labels)\n",
    "    pred_silhouette = silhouette_score(pred_adt_adata.X, pred_labels)\n",
    "    \n",
    "    print(f\"\\nSilhouette score for true clusters: {true_silhouette:.4f}\")\n",
    "    print(f\"Silhouette score for predicted clusters: {pred_silhouette:.4f}\")\n",
    "    print(f\"Ratio (pred/true): {pred_silhouette/true_silhouette:.4f}\")\n",
    "    if pred_silhouette >= true_silhouette:\n",
    "        print(\"The predicted embeddings have equally good or better defined clusters than the true embeddings.\")\n",
    "    else:\n",
    "        print(\"The true embeddings have better defined clusters than the predicted embeddings.\")\n",
    "except:\n",
    "    print(\"Could not calculate silhouette scores, possibly due to cluster numbers or sample size.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d56994e",
   "metadata": {},
   "source": [
    "## 16. Interpretation of Cluster Preservation Results\n",
    "\n",
    "The above analysis helps us understand how well our Transformer Encoder model preserves biological cell types when mapping from RNA to ADT embeddings. Here's how to interpret these results:\n",
    "\n",
    "### Key Metrics:\n",
    "- **Adjusted Rand Index (ARI)**: Measures the similarity between the true and predicted cluster assignments, adjusted for chance. Values range from -1 to 1, where 1 indicates perfect agreement, and values near 0 indicate random clustering.\n",
    "- **Normalized Mutual Information (NMI)**: Quantifies the shared information between the two clusterings. Values range from 0 to 1, with 1 indicating perfect agreement.\n",
    "- **Procrustes Disparity**: Measures how well the global structure is preserved after optimal alignment (lower is better).\n",
    "- **Silhouette Scores**: Measures how well-defined the clusters are (higher is better).\n",
    "\n",
    "### Interpretation Guidelines:\n",
    "\n",
    "1. **Strong Cluster Preservation** (Good model performance):\n",
    "   - High ARI (> 0.7) and NMI (> 0.8)\n",
    "   - Similar number of clusters between true and predicted embeddings\n",
    "   - Low Procrustes disparity\n",
    "   - Similar silhouette scores between true and predicted embeddings\n",
    "   - Clear diagonal pattern in confusion matrix\n",
    "\n",
    "2. **Moderate Cluster Preservation** (Acceptable model performance):\n",
    "   - Moderate ARI (0.4-0.7) and NMI (0.5-0.8)\n",
    "   - Some differences in cluster numbers\n",
    "   - Some off-diagonal elements in confusion matrix, but still showing structure\n",
    "   - Visible separation of clusters in UMAP visualizations, though not identical\n",
    "\n",
    "3. **Poor Cluster Preservation** (Model needs improvement):\n",
    "   - Low ARI (< 0.4) and NMI (< 0.5)\n",
    "   - Very different number of clusters\n",
    "   - No clear pattern in confusion matrix\n",
    "   - Poor separation in UMAP visualizations\n",
    "\n",
    "### Biological Significance:\n",
    "If the model demonstrates good cluster preservation, it suggests that:\n",
    "1. The RNA expression data contains sufficient information to predict cell types as defined by surface protein markers\n",
    "2. The transformer model has successfully learned to map between these two modalities\n",
    "3. The predicted ADT embeddings could potentially be used for downstream analyses in place of actual ADT measurements\n",
    "\n",
    "This evaluation framework provides a comprehensive assessment of how well the model maintains biological cell type structure when mapping between RNA and ADT modalities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc402109",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook implements a pipeline to learn mappings between GAT embeddings of RNA and ADT data:\n",
    "\n",
    "1. **Data Preprocessing**: Both RNA and ADT data are normalized, scaled, and processed to create neighbor graphs\n",
    "2. **GAT Training**: Separate GAT models are trained on RNA and ADT data for node classification\n",
    "3. **Embedding Extraction**: Intermediate embeddings are extracted from the trained GAT models\n",
    "4. **Transformer Mapping**: \n",
    "   - A Transformer Encoder architecture learns to map RNA embeddings to ADT embeddings\n",
    "   - Self-attention mechanisms capture complex dependencies between embedding dimensions\n",
    "   - Multi-headed attention allows the model to focus on different aspects of the data\n",
    "5. **Evaluation**: The mapping quality is assessed using MSE, RÂ², and correlation metrics\n",
    "\n",
    "The Transformer architecture offers several advantages over MLPs:\n",
    "- **Context awareness**: Self-attention mechanism captures global dependencies in the embeddings\n",
    "- **Parameter efficiency**: Shared parameters across layers for better generalization\n",
    "- **Representation power**: Better handling of complex relationships between features\n",
    "\n",
    "The trained models can be used to predict ADT embeddings from RNA data, enabling cross-modal analysis and integration."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
