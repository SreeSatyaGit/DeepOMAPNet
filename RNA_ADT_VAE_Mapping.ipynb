{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1a7d744",
   "metadata": {},
   "source": [
    "# VAE Mapping between GAT Embeddings of RNA and ADT\n",
    "\n",
    "This notebook learns a mapping between GAT embeddings from RNA data and GAT embeddings from ADT data using a Variational Autoencoder (VAE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60202d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory optimization and system check\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Set memory management environment variables\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "# Check system resources\n",
    "print(\"=== System Resources ===\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.cuda.current_device()\n",
    "    gpu_props = torch.cuda.get_device_properties(device)\n",
    "    total_memory = gpu_props.total_memory / (1024**3)  # Convert to GB\n",
    "    \n",
    "    print(f\"GPU: {gpu_props.name}\")\n",
    "    print(f\"Total GPU Memory: {total_memory:.1f} GB\")\n",
    "    print(f\"GPU Compute Capability: {gpu_props.major}.{gpu_props.minor}\")\n",
    "    \n",
    "    # Clear any cached memory\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Check current memory usage\n",
    "    allocated = torch.cuda.memory_allocated(device) / (1024**3)\n",
    "    reserved = torch.cuda.memory_reserved(device) / (1024**3)\n",
    "    \n",
    "    print(f\"Currently allocated: {allocated:.2f} GB\")\n",
    "    print(f\"Currently reserved: {reserved:.2f} GB\")\n",
    "    print(f\"Available: {total_memory - reserved:.2f} GB\")\n",
    "    \n",
    "    # Set recommendations based on available memory\n",
    "    if total_memory < 8:\n",
    "        print(\"\\nâš ï¸  WARNING: Low GPU memory detected!\")\n",
    "        print(\"Recommendations:\")\n",
    "        print(\"- Use CPU fallback if needed\")\n",
    "        print(\"- Reduce batch sizes\")\n",
    "        print(\"- Use graph sparsification\")\n",
    "    elif total_memory < 16:\n",
    "        print(\"\\nðŸ’¡ Moderate GPU memory - will use optimized settings\")\n",
    "    else:\n",
    "        print(\"\\nâœ… Sufficient GPU memory available\")\n",
    "        \n",
    "else:\n",
    "    print(\"CUDA not available - will use CPU\")\n",
    "    print(\"Note: Training will be slower but should work with larger graphs\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5576248",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Set environment variables for better memory management\n",
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import pandas as pd\n",
    "\n",
    "import scanpy as sc\n",
    "import scanpy.external as sce\n",
    "from scipy import sparse\n",
    "\n",
    "from DeepOMAPNet.Preprocess import prepare_train_test_anndata\n",
    "\n",
    "# Set memory management\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current GPU: {torch.cuda.current_device()}\")\n",
    "    print(f\"GPU name: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.1f} GB\")\n",
    "else:\n",
    "    print(\"CUDA not available, using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891322f5",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4aa991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessed data\n",
    "data = prepare_train_test_anndata()\n",
    "trainGene = data[0]  # RNA data\n",
    "trainADT = data[2]   # ADT data\n",
    "\n",
    "print(f\"RNA data shape: {trainGene.shape}\")\n",
    "print(f\"ADT data shape: {trainADT.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d918a2d",
   "metadata": {},
   "source": [
    "## 2. Preprocess RNA Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2516c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNA preprocessing\n",
    "sc.pp.normalize_total(trainGene, target_sum=1e4)\n",
    "sc.pp.log1p(trainGene)\n",
    "sc.pp.highly_variable_genes(trainGene, n_top_genes=2000, batch_key=\"samples\")\n",
    "trainGene = trainGene[:, trainGene.var.highly_variable].copy()\n",
    "\n",
    "sc.pp.scale(trainGene, max_value=10)\n",
    "sc.tl.pca(trainGene, n_comps=50, svd_solver=\"arpack\")\n",
    "\n",
    "# Build neighbor graph for RNA\n",
    "sc.pp.neighbors(trainGene, n_neighbors=15, n_pcs=50)\n",
    "sc.tl.leiden(trainGene, resolution=1.0)\n",
    "\n",
    "print(f\"RNA data after preprocessing: {trainGene.shape}\")\n",
    "print(f\"Number of RNA clusters: {trainGene.obs['leiden'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80d7bba",
   "metadata": {},
   "source": [
    "## 3. Preprocess ADT Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bf153a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADT preprocessing\n",
    "sc.pp.normalize_total(trainADT, target_sum=1e4)\n",
    "sc.pp.log1p(trainADT)\n",
    "sc.pp.scale(trainADT, max_value=10)\n",
    "sc.tl.pca(trainADT, n_comps=50, svd_solver=\"arpack\")\n",
    "\n",
    "# Build neighbor graph for ADT using BBKNN for batch correction\n",
    "sce.pp.bbknn(\n",
    "    trainADT,\n",
    "    batch_key='samples',\n",
    "    n_pcs=50,\n",
    "    neighbors_within_batch=3,\n",
    "    trim=0\n",
    ")\n",
    "\n",
    "sc.tl.leiden(trainADT, resolution=1.0)\n",
    "\n",
    "print(f\"ADT data after preprocessing: {trainADT.shape}\")\n",
    "print(f\"Number of ADT clusters: {trainADT.obs['leiden'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6513723",
   "metadata": {},
   "source": [
    "## 4. Build PyTorch Geometric Data Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b24abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparsify_graph(adata, max_edges_per_node=50):\n",
    "    \"\"\"Sparsify the graph by keeping only top k neighbors per node\"\"\"\n",
    "    \n",
    "    A = adata.obsp[\"connectivities\"].tocsr()\n",
    "    n_nodes = A.shape[0]\n",
    "    \n",
    "    # Check if sparsification is needed\n",
    "    avg_degree = A.nnz / n_nodes\n",
    "    if avg_degree <= max_edges_per_node:\n",
    "        print(f\"Graph already sparse enough (avg degree: {avg_degree:.1f})\")\n",
    "        return adata\n",
    "    \n",
    "    print(f\"Sparsifying graph from avg degree {avg_degree:.1f} to max {max_edges_per_node}\")\n",
    "    \n",
    "    # Create new sparse matrix\n",
    "    row_indices = []\n",
    "    col_indices = []\n",
    "    data_values = []\n",
    "    \n",
    "    for i in range(n_nodes):\n",
    "        # Get neighbors and their weights for node i\n",
    "        start_idx = A.indptr[i]\n",
    "        end_idx = A.indptr[i + 1]\n",
    "        neighbors = A.indices[start_idx:end_idx]\n",
    "        weights = A.data[start_idx:end_idx]\n",
    "        \n",
    "        # Keep only top k neighbors\n",
    "        if len(neighbors) > max_edges_per_node:\n",
    "            top_k_indices = np.argpartition(weights, -max_edges_per_node)[-max_edges_per_node:]\n",
    "            neighbors = neighbors[top_k_indices]\n",
    "            weights = weights[top_k_indices]\n",
    "        \n",
    "        # Add edges\n",
    "        row_indices.extend([i] * len(neighbors))\n",
    "        col_indices.extend(neighbors)\n",
    "        data_values.extend(weights)\n",
    "    \n",
    "    # Create new adjacency matrix\n",
    "    A_sparse = sparse.csr_matrix(\n",
    "        (data_values, (row_indices, col_indices)), \n",
    "        shape=(n_nodes, n_nodes)\n",
    "    )\n",
    "    \n",
    "    # Make symmetric\n",
    "    A_sparse = (A_sparse + A_sparse.T) / 2\n",
    "    \n",
    "    # Update the AnnData object\n",
    "    adata.obsp[\"connectivities\"] = A_sparse\n",
    "    \n",
    "    new_avg_degree = A_sparse.nnz / n_nodes\n",
    "    print(f\"New average degree: {new_avg_degree:.1f}\")\n",
    "    \n",
    "    return adata\n",
    "\n",
    "def build_pyg_data(adata, use_pca=True, sparsify_large_graphs=True, max_edges_per_node=50):\n",
    "    \"\"\"Build PyTorch Geometric Data object from AnnData\"\"\"\n",
    "    \n",
    "    # Sparsify if needed\n",
    "    if sparsify_large_graphs:\n",
    "        A = adata.obsp[\"connectivities\"]\n",
    "        avg_degree = A.nnz / A.shape[0]\n",
    "        if avg_degree > max_edges_per_node:\n",
    "            print(f\"Large graph detected (avg degree: {avg_degree:.1f}), applying sparsification...\")\n",
    "            adata = sparsify_graph(adata, max_edges_per_node)\n",
    "    \n",
    "    # Features\n",
    "    X = adata.obsm[\"X_pca\"] if use_pca else adata.X.toarray()\n",
    "    \n",
    "    # Labels (leiden clusters)\n",
    "    y = adata.obs[\"leiden\"].astype(int).to_numpy()\n",
    "    \n",
    "    # Edge index from connectivities\n",
    "    A = adata.obsp[\"connectivities\"].tocsr()\n",
    "    A_triu = sparse.triu(A, k=1)\n",
    "    row, col = A_triu.nonzero()\n",
    "    edge_index = torch.tensor(np.vstack([row, col]), dtype=torch.long)\n",
    "    \n",
    "    # Create PyG Data object\n",
    "    data = Data(\n",
    "        x=torch.tensor(X, dtype=torch.float32),\n",
    "        edge_index=edge_index,\n",
    "        y=torch.tensor(y, dtype=torch.long),\n",
    "    )\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Build data objects with memory optimization\n",
    "print(\"Building PyG data objects...\")\n",
    "\n",
    "# Check available GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # GB\n",
    "    print(f\"Available GPU memory: {gpu_memory:.1f} GB\")\n",
    "    \n",
    "    # Estimate memory requirements\n",
    "    rna_edges = trainGene.obsp[\"connectivities\"].nnz\n",
    "    adt_edges = trainADT.obsp[\"connectivities\"].nnz\n",
    "    \n",
    "    print(f\"RNA graph edges: {rna_edges:,}\")\n",
    "    print(f\"ADT graph edges: {adt_edges:,}\")\n",
    "    \n",
    "    # Set sparsification based on graph size\n",
    "    max_edges_rna = 100 if rna_edges > 5000000 else 200\n",
    "    max_edges_adt = 50 if adt_edges > 10000000 else 100\n",
    "    \n",
    "    print(f\"Using max edges per node - RNA: {max_edges_rna}, ADT: {max_edges_adt}\")\n",
    "else:\n",
    "    print(\"Using CPU - no memory constraints\")\n",
    "    max_edges_rna = 200\n",
    "    max_edges_adt = 100\n",
    "\n",
    "# Build data objects\n",
    "rna_data = build_pyg_data(trainGene, use_pca=True, sparsify_large_graphs=True, max_edges_per_node=max_edges_rna)\n",
    "adt_data = build_pyg_data(trainADT, use_pca=True, sparsify_large_graphs=True, max_edges_per_node=max_edges_adt)\n",
    "\n",
    "print(f\"RNA PyG data - Nodes: {rna_data.num_nodes}, Edges: {rna_data.num_edges}, Features: {rna_data.num_node_features}\")\n",
    "print(f\"ADT PyG data - Nodes: {adt_data.num_nodes}, Edges: {adt_data.num_edges}, Features: {adt_data.num_node_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbbad62",
   "metadata": {},
   "source": [
    "## 5. Define GAT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08e14b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGAT(torch.nn.Module):\n",
    "    \"\"\"Simplified GAT for memory-constrained scenarios\"\"\"\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, heads=4, dropout=0.6):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # Single GAT layer for memory efficiency\n",
    "        self.conv1 = GATConv(in_channels, out_channels, heads=heads, dropout=dropout, concat=False)\n",
    "        \n",
    "    def forward(self, x, edge_index, return_embeddings=False):\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.conv1(x, edge_index)\n",
    "        \n",
    "        if return_embeddings:\n",
    "            return x\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def get_embeddings(self, x, edge_index):\n",
    "        \"\"\"Get embeddings for mapping\"\"\"\n",
    "        return self.forward(x, edge_index, return_embeddings=True)\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, heads=8, dropout=0.6):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.conv1 = GATConv(in_channels, hidden_channels, heads=heads, dropout=dropout)\n",
    "        self.conv2 = GATConv(hidden_channels * heads, out_channels, heads=1, dropout=dropout)\n",
    "        \n",
    "    def forward(self, x, edge_index, return_embeddings=False):\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        \n",
    "        if return_embeddings:\n",
    "            return x  # Return embeddings before final layer\n",
    "            \n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "    def get_embeddings(self, x, edge_index):\n",
    "        \"\"\"Get intermediate embeddings for mapping\"\"\"\n",
    "        return self.forward(x, edge_index, return_embeddings=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd5aada",
   "metadata": {},
   "source": [
    "## 6. Train GAT Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3021eeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gat_model(data, model_name=\"GAT\", epochs=200, use_cpu_fallback=False):\n",
    "    \"\"\"Train a GAT model and return the trained model\"\"\"\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() and not use_cpu_fallback else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Check memory requirements and adjust accordingly\n",
    "    num_edges = data.num_edges\n",
    "    num_nodes = data.num_nodes\n",
    "    \n",
    "    print(f\"Graph stats - Nodes: {num_nodes}, Edges: {num_edges}\")\n",
    "    \n",
    "    # Memory optimization: reduce model size if too many edges\n",
    "    use_simple_model = False\n",
    "    if num_edges > 2000000:  # If more than 2M edges\n",
    "        print(\"Very large graph detected, using simplified GAT architecture...\")\n",
    "        hidden_dim = 32\n",
    "        heads = 4\n",
    "        use_simple_model = True\n",
    "    elif num_edges > 1000000:  # If more than 1M edges\n",
    "        print(\"Large graph detected, reducing model complexity...\")\n",
    "        hidden_dim = 32\n",
    "        heads = 4\n",
    "    else:\n",
    "        hidden_dim = 64\n",
    "        heads = 8\n",
    "    \n",
    "    # Create train/val/test masks\n",
    "    N = data.num_nodes\n",
    "    y_np = data.y.cpu().numpy()\n",
    "    \n",
    "    from sklearn.model_selection import StratifiedShuffleSplit\n",
    "    \n",
    "    # Split 80/10/10\n",
    "    sss1 = StratifiedShuffleSplit(n_splits=1, train_size=0.8, random_state=42)\n",
    "    train_idx, temp_idx = next(sss1.split(np.zeros(N), y_np))\n",
    "    \n",
    "    y_temp = y_np[temp_idx]\n",
    "    sss2 = StratifiedShuffleSplit(n_splits=1, train_size=0.5, random_state=43)\n",
    "    val_rel, test_rel = next(sss2.split(np.zeros(len(temp_idx)), y_temp))\n",
    "    val_idx = temp_idx[val_rel]\n",
    "    test_idx = temp_idx[test_rel]\n",
    "    \n",
    "    train_mask = torch.zeros(N, dtype=torch.bool)\n",
    "    val_mask = torch.zeros(N, dtype=torch.bool)\n",
    "    test_mask = torch.zeros(N, dtype=torch.bool)\n",
    "    train_mask[train_idx] = True\n",
    "    val_mask[val_idx] = True\n",
    "    test_mask[test_idx] = True\n",
    "    \n",
    "    data.train_mask = train_mask\n",
    "    data.val_mask = val_mask\n",
    "    data.test_mask = test_mask\n",
    "    \n",
    "    # Initialize model\n",
    "    in_dim = data.x.size(1)\n",
    "    n_class = int(data.y.max().item() + 1)\n",
    "    \n",
    "    if use_simple_model:\n",
    "        model = SimpleGAT(in_dim, hidden_dim, n_class, heads=heads).to(device)\n",
    "        print(f\"Using SimpleGAT: {in_dim} -> {n_class} (hidden: {hidden_dim}, heads: {heads})\")\n",
    "    else:\n",
    "        model = GAT(in_dim, hidden_dim, n_class, heads=heads).to(device)\n",
    "        print(f\"Using GAT: {in_dim} -> {hidden_dim} -> {n_class} (heads: {heads})\")\n",
    "    \n",
    "    # Move data to device with memory management\n",
    "    cpu_fallback_triggered = False\n",
    "    try:\n",
    "        data = data.to(device)\n",
    "        print(f\"Successfully moved data to {device}\")\n",
    "    except RuntimeError as e:\n",
    "        if \"out of memory\" in str(e).lower():\n",
    "            print(f\"GPU memory insufficient, falling back to CPU...\")\n",
    "            device = torch.device('cpu')\n",
    "            model = model.cpu()\n",
    "            data = data.cpu()\n",
    "            cpu_fallback_triggered = True\n",
    "        else:\n",
    "            raise e\n",
    "    \n",
    "    # Initialize optimizer and criterion\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    def train():\n",
    "        nonlocal model, data, optimizer, device, cpu_fallback_triggered\n",
    "        \n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        try:\n",
    "            if device.type == 'cuda':\n",
    "                torch.cuda.empty_cache()  # Clear cache before forward pass\n",
    "            \n",
    "            out = model(data.x, data.edge_index)\n",
    "            loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if device.type == 'cuda':\n",
    "                torch.cuda.empty_cache()  # Clear cache after backward pass\n",
    "                \n",
    "            return loss\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e).lower() and not cpu_fallback_triggered:\n",
    "                print(f\"GPU OOM during training, switching to CPU...\")\n",
    "                # Move everything to CPU\n",
    "                device = torch.device('cpu')\n",
    "                model = model.cpu()\n",
    "                data = data.cpu()\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "                cpu_fallback_triggered = True\n",
    "                \n",
    "                # Retry the forward pass on CPU\n",
    "                optimizer.zero_grad()\n",
    "                out = model(data.x, data.edge_index)\n",
    "                loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                return loss\n",
    "            else:\n",
    "                raise e\n",
    "    \n",
    "    def test(mask):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            if device.type == 'cuda':\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "            out = model(data.x, data.edge_index)\n",
    "            pred = out.argmax(dim=1)\n",
    "            correct = pred[mask] == data.y[mask]\n",
    "            acc = int(correct.sum()) / int(mask.sum())\n",
    "            return acc\n",
    "    \n",
    "    print(f\"Training {model_name} model...\")\n",
    "    best_val_acc = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        loss = train()\n",
    "        \n",
    "        if epoch % 50 == 0 or epoch == 1:\n",
    "            val_acc = test(data.val_mask)\n",
    "            test_acc = test(data.test_mask)\n",
    "            print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_acc:.4f}, Test: {test_acc:.4f}')\n",
    "            \n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                best_model_state = model.state_dict().copy()\n",
    "    \n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    final_test_acc = test(data.test_mask)\n",
    "    print(f\"Final {model_name} test accuracy: {final_test_acc:.4f}\")\n",
    "    \n",
    "    return model, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7aaf416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train GAT models for both RNA and ADT with memory management\n",
    "print(\"=== Training RNA GAT ===\")\n",
    "try:\n",
    "    rna_gat_model, rna_data_with_masks = train_gat_model(rna_data, \"RNA GAT\", epochs=200)\n",
    "except RuntimeError as e:\n",
    "    if \"out of memory\" in str(e).lower():\n",
    "        print(\"GPU memory insufficient for RNA GAT, trying CPU...\")\n",
    "        rna_gat_model, rna_data_with_masks = train_gat_model(rna_data, \"RNA GAT\", epochs=200, use_cpu_fallback=True)\n",
    "    else:\n",
    "        raise e\n",
    "\n",
    "print(\"\\n=== Training ADT GAT ===\")\n",
    "try:\n",
    "    adt_gat_model, adt_data_with_masks = train_gat_model(adt_data, \"ADT GAT\", epochs=200)\n",
    "except RuntimeError as e:\n",
    "    if \"out of memory\" in str(e).lower():\n",
    "        print(\"GPU memory insufficient for ADT GAT, trying CPU...\")\n",
    "        adt_gat_model, adt_data_with_masks = train_gat_model(adt_data, \"ADT GAT\", epochs=200, use_cpu_fallback=True)\n",
    "    else:\n",
    "        raise e\n",
    "\n",
    "print(\"\\n=== GAT Training Complete ===\")\n",
    "print(f\"RNA GAT model trained successfully\")\n",
    "print(f\"ADT GAT model trained successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5ae7a9",
   "metadata": {},
   "source": [
    "## 7. Extract GAT Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba6180f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings(model, data):\n",
    "    \"\"\"Extract embeddings from trained GAT model\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Ensure model and data are on the same device\n",
    "    device = next(model.parameters()).device\n",
    "    if data.x.device != device:\n",
    "        print(f\"Moving data from {data.x.device} to {device}\")\n",
    "        data = data.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Clear cache if using GPU\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        embeddings = model.get_embeddings(data.x, data.edge_index)\n",
    "        \n",
    "        # Move to CPU for further processing\n",
    "        embeddings = embeddings.cpu()\n",
    "        \n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "    return embeddings\n",
    "\n",
    "# Clear any existing cache\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Extract embeddings\n",
    "print(\"Extracting embeddings...\")\n",
    "rna_embeddings = extract_embeddings(rna_gat_model, rna_data_with_masks)\n",
    "adt_embeddings = extract_embeddings(adt_gat_model, adt_data_with_masks)\n",
    "\n",
    "print(f\"RNA embeddings shape: {rna_embeddings.shape}\")\n",
    "print(f\"ADT embeddings shape: {adt_embeddings.shape}\")\n",
    "\n",
    "# Ensure both embeddings have the same number of cells\n",
    "assert rna_embeddings.shape[0] == adt_embeddings.shape[0], \"Number of cells must match\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa76166d",
   "metadata": {},
   "source": [
    "## 8. Define VAE Mapping Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893507b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAEMapping(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, latent_dim=32, hidden_dims=[256, 128]):\n",
    "        super(VAEMapping, self).__init__()\n",
    "        \n",
    "        # Encoder layers\n",
    "        encoder_layers = []\n",
    "        current_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            encoder_layers.extend([\n",
    "                nn.Linear(current_dim, hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.3)\n",
    "            ])\n",
    "            current_dim = hidden_dim\n",
    "        \n",
    "        # Mean and variance layers for the latent distribution\n",
    "        self.fc_mu = nn.Linear(current_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(current_dim, latent_dim)\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(*encoder_layers)\n",
    "        \n",
    "        # Decoder layers\n",
    "        decoder_layers = []\n",
    "        current_dim = latent_dim\n",
    "        \n",
    "        # Reverse the hidden dimensions for the decoder\n",
    "        for hidden_dim in reversed(hidden_dims):\n",
    "            decoder_layers.extend([\n",
    "                nn.Linear(current_dim, hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.3)\n",
    "            ])\n",
    "            current_dim = hidden_dim\n",
    "        \n",
    "        # Output layer\n",
    "        decoder_layers.append(nn.Linear(current_dim, output_dim))\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(*decoder_layers)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        \"\"\"Encode input to get mean and log variance of the latent distribution\"\"\"\n",
    "        x = self.encoder(x)\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \"\"\"Reparameterization trick to sample from latent distribution\"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu + eps * std\n",
    "        return z\n",
    "        \n",
    "    def decode(self, z):\n",
    "        \"\"\"Decode latent representation to output space\"\"\"\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the VAE\"\"\"\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_recon = self.decode(z)\n",
    "        return x_recon, mu, logvar\n",
    "    \n",
    "    def sample(self, num_samples, device):\n",
    "        \"\"\"Generate samples from the latent space\"\"\"\n",
    "        z = torch.randn(num_samples, self.fc_mu.out_features).to(device)\n",
    "        samples = self.decode(z)\n",
    "        return samples\n",
    "\n",
    "# Initialize VAE mapping model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "input_dim = rna_embeddings.shape[1]\n",
    "output_dim = adt_embeddings.shape[1]\n",
    "latent_dim = 32  # Dimension of the latent space\n",
    "\n",
    "vae_model = VAEMapping(input_dim, output_dim, latent_dim=latent_dim).to(device)\n",
    "\n",
    "print(f\"VAE Model: {input_dim} -> {latent_dim} -> {output_dim}\")\n",
    "print(vae_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b5a270",
   "metadata": {},
   "source": [
    "## 9. Prepare Training Data for VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c80cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert embeddings to CPU and numpy\n",
    "rna_emb_np = rna_embeddings.cpu().numpy()\n",
    "adt_emb_np = adt_embeddings.cpu().numpy()\n",
    "\n",
    "# Split data for VAE training (use same train/val/test split as GAT)\n",
    "train_mask_np = rna_data_with_masks.train_mask.cpu().numpy()\n",
    "val_mask_np = rna_data_with_masks.val_mask.cpu().numpy()\n",
    "test_mask_np = rna_data_with_masks.test_mask.cpu().numpy()\n",
    "\n",
    "# Prepare training data\n",
    "X_train = torch.tensor(rna_emb_np[train_mask_np], dtype=torch.float32)\n",
    "y_train = torch.tensor(adt_emb_np[train_mask_np], dtype=torch.float32)\n",
    "\n",
    "X_val = torch.tensor(rna_emb_np[val_mask_np], dtype=torch.float32)\n",
    "y_val = torch.tensor(adt_emb_np[val_mask_np], dtype=torch.float32)\n",
    "\n",
    "X_test = torch.tensor(rna_emb_np[test_mask_np], dtype=torch.float32)\n",
    "y_test = torch.tensor(adt_emb_np[test_mask_np], dtype=torch.float32)\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 128\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cc89d5",
   "metadata": {},
   "source": [
    "## 10. Train VAE Mapping Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956991a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss_function(recon_x, x, mu, logvar, kl_weight=0.01):\n",
    "    \"\"\"\n",
    "    VAE loss function with KL divergence and reconstruction loss\n",
    "    \n",
    "    Args:\n",
    "        recon_x: Reconstructed output\n",
    "        x: Target output\n",
    "        mu: Mean of latent distribution\n",
    "        logvar: Log variance of latent distribution\n",
    "        kl_weight: Weight of the KL divergence term\n",
    "    \"\"\"\n",
    "    # Reconstruction loss (MSE)\n",
    "    mse_loss = F.mse_loss(recon_x, x, reduction='sum')\n",
    "    \n",
    "    # KL divergence\n",
    "    kld_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    \n",
    "    # Total loss = reconstruction loss + weighted KL divergence\n",
    "    total_loss = mse_loss + kl_weight * kld_loss\n",
    "    \n",
    "    return total_loss, mse_loss, kld_loss\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 300\n",
    "learning_rate = 0.001\n",
    "weight_decay = 1e-5\n",
    "kl_weight_start = 0.0001  # Start with small KL weight\n",
    "kl_weight_end = 0.01      # End with larger KL weight\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(vae_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=20)\n",
    "\n",
    "# Training loop\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "mse_losses = []\n",
    "kl_losses = []\n",
    "best_val_loss = float('inf')\n",
    "best_model_state = None\n",
    "patience = 50\n",
    "patience_counter = 0\n",
    "\n",
    "print(\"Training VAE mapping model...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Update KL weight with annealing schedule\n",
    "    kl_weight = kl_weight_start + (kl_weight_end - kl_weight_start) * min(1.0, epoch / 100)\n",
    "    \n",
    "    # Training phase\n",
    "    vae_model.train()\n",
    "    train_loss = 0.0\n",
    "    train_mse = 0.0\n",
    "    train_kl = 0.0\n",
    "    \n",
    "    for batch_x, batch_y in train_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = vae_model(batch_x)\n",
    "        \n",
    "        loss, mse, kl = vae_loss_function(recon_batch, batch_y, mu, logvar, kl_weight)\n",
    "        loss = loss / len(batch_x)  # Normalize by batch size\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * len(batch_x)\n",
    "        train_mse += mse.item()\n",
    "        train_kl += kl.item()\n",
    "    \n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    train_mse /= len(train_loader.dataset)\n",
    "    train_kl /= len(train_loader.dataset)\n",
    "    train_losses.append(train_loss)\n",
    "    mse_losses.append(train_mse)\n",
    "    kl_losses.append(train_kl)\n",
    "    \n",
    "    # Validation phase\n",
    "    vae_model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_mse = 0.0\n",
    "    val_kl = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in val_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            recon_batch, mu, logvar = vae_model(batch_x)\n",
    "            loss, mse, kl = vae_loss_function(recon_batch, batch_y, mu, logvar, kl_weight)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            val_mse += mse.item()\n",
    "            val_kl += kl.item()\n",
    "    \n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_mse /= len(val_loader.dataset)\n",
    "    val_kl /= len(val_loader.dataset)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = vae_model.state_dict().copy()\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    if (epoch + 1) % 25 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}')\n",
    "        print(f'  MSE: {train_mse:.6f}, KL: {train_kl:.6f}, KL Weight: {kl_weight:.6f}')\n",
    "    \n",
    "    if patience_counter >= patience:\n",
    "        print(f'Early stopping at epoch {epoch+1}')\n",
    "        break\n",
    "\n",
    "# Load best model\n",
    "vae_model.load_state_dict(best_model_state)\n",
    "print(f'Best validation loss: {best_val_loss:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5622121d",
   "metadata": {},
   "source": [
    "## 11. Evaluate VAE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9cba6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "vae_model.eval()\n",
    "test_loss = 0.0\n",
    "predictions = []\n",
    "ground_truth = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y in test_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        recon_batch, mu, logvar = vae_model(batch_x)\n",
    "        \n",
    "        # Only compute reconstruction loss for evaluation\n",
    "        loss = F.mse_loss(recon_batch, batch_y)\n",
    "        test_loss += loss.item() * len(batch_x)\n",
    "        \n",
    "        predictions.append(recon_batch.cpu().numpy())\n",
    "        ground_truth.append(batch_y.cpu().numpy())\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "predictions = np.vstack(predictions)\n",
    "ground_truth = np.vstack(ground_truth)\n",
    "\n",
    "# Calculate metrics\n",
    "mse = mean_squared_error(ground_truth, predictions)\n",
    "r2 = r2_score(ground_truth, predictions)\n",
    "\n",
    "# Calculate correlation per dimension\n",
    "pearson_corrs = []\n",
    "spearman_corrs = []\n",
    "\n",
    "for i in range(ground_truth.shape[1]):\n",
    "    pearson_r, _ = pearsonr(ground_truth[:, i], predictions[:, i])\n",
    "    spearman_r, _ = spearmanr(ground_truth[:, i], predictions[:, i])\n",
    "    pearson_corrs.append(pearson_r)\n",
    "    spearman_corrs.append(spearman_r)\n",
    "\n",
    "mean_pearson = np.mean(pearson_corrs)\n",
    "mean_spearman = np.mean(spearman_corrs)\n",
    "\n",
    "print(f\"\\n=== VAE Mapping Results ===\")\n",
    "print(f\"Test Loss (MSE): {test_loss:.6f}\")\n",
    "print(f\"MSE: {mse:.6f}\")\n",
    "print(f\"RÂ² Score: {r2:.4f}\")\n",
    "print(f\"Mean Pearson Correlation: {mean_pearson:.4f}\")\n",
    "print(f\"Mean Spearman Correlation: {mean_spearman:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4faa4137",
   "metadata": {},
   "source": [
    "## 12. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4a7b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "plt.figure(figsize=(16, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Total Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(mse_losses, label='MSE Loss')\n",
    "plt.plot(kl_losses, label='KL Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss Component')\n",
    "plt.title('MSE vs KL Divergence Loss')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.hist(pearson_corrs, bins=20, alpha=0.7, label='Pearson')\n",
    "plt.hist(spearman_corrs, bins=20, alpha=0.7, label='Spearman')\n",
    "plt.xlabel('Correlation')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Per-dimension Correlation Distribution')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2794124d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plots for first few dimensions\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(min(6, ground_truth.shape[1])):\n",
    "    ax = axes[i]\n",
    "    ax.scatter(ground_truth[:, i], predictions[:, i], alpha=0.6, s=1)\n",
    "    \n",
    "    # Add perfect prediction line\n",
    "    min_val = min(ground_truth[:, i].min(), predictions[:, i].min())\n",
    "    max_val = max(ground_truth[:, i].max(), predictions[:, i].max())\n",
    "    ax.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel(f'True ADT Embedding Dim {i+1}')\n",
    "    ax.set_ylabel(f'Predicted ADT Embedding Dim {i+1}')\n",
    "    ax.set_title(f'Dim {i+1}: r={pearson_corrs[i]:.3f}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf208f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the latent space using t-SNE\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Sample a subset of data points for t-SNE (can be slow on large datasets)\n",
    "sample_size = min(1000, len(X_test))\n",
    "indices = np.random.choice(len(X_test), sample_size, replace=False)\n",
    "\n",
    "X_test_sample = X_test[indices].to(device)\n",
    "y_test_sample = y_test[indices].to(device)\n",
    "\n",
    "# Get embeddings from the VAE encoder\n",
    "with torch.no_grad():\n",
    "    mu, _ = vae_model.encode(X_test_sample)\n",
    "    mu = mu.cpu().numpy()\n",
    "\n",
    "# Compute t-SNE embedding\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "tsne_embedding = tsne.fit_transform(mu)\n",
    "\n",
    "# Use cluster labels if available, otherwise create artificial ones for visualization\n",
    "if hasattr(rna_data_with_masks, 'y'):\n",
    "    cluster_labels = rna_data_with_masks.y.cpu().numpy()[test_mask_np][indices]\n",
    "else:\n",
    "    # Generate random labels or use K-means for visualization\n",
    "    from sklearn.cluster import KMeans\n",
    "    kmeans = KMeans(n_clusters=8, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(mu)\n",
    "\n",
    "# Plot the t-SNE visualization\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(tsne_embedding[:, 0], tsne_embedding[:, 1], \n",
    "                      c=cluster_labels, cmap='tab10', s=5, alpha=0.8)\n",
    "\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.title('t-SNE Visualization of VAE Latent Space')\n",
    "plt.xlabel('t-SNE Dimension 1')\n",
    "plt.ylabel('t-SNE Dimension 2')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a256421",
   "metadata": {},
   "source": [
    "## 13. Save Models and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71a5aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained models\n",
    "torch.save({\n",
    "    'rna_gat_state_dict': rna_gat_model.state_dict(),\n",
    "    'adt_gat_state_dict': adt_gat_model.state_dict(),\n",
    "    'vae_mapping_state_dict': vae_model.state_dict(),\n",
    "    'rna_input_dim': input_dim,\n",
    "    'adt_output_dim': output_dim,\n",
    "    'latent_dim': latent_dim,\n",
    "    'test_results': {\n",
    "        'mse': mse,\n",
    "        'r2': r2,\n",
    "        'mean_pearson': mean_pearson,\n",
    "        'mean_spearman': mean_spearman,\n",
    "        'pearson_corrs': pearson_corrs,\n",
    "        'spearman_corrs': spearman_corrs\n",
    "    }\n",
    "}, 'rna_adt_vae_mapping_models.pth')\n",
    "\n",
    "print(\"Models and results saved to 'rna_adt_vae_mapping_models.pth'\")\n",
    "\n",
    "# Save predictions for further analysis\n",
    "np.savez('vae_mapping_predictions.npz', \n",
    "         predictions=predictions, \n",
    "         ground_truth=ground_truth,\n",
    "         pearson_corrs=pearson_corrs,\n",
    "         spearman_corrs=spearman_corrs)\n",
    "\n",
    "print(\"Predictions saved to 'vae_mapping_predictions.npz'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8b84b7",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook implements a pipeline to learn mappings between GAT embeddings of RNA and ADT data using a Variational Autoencoder (VAE):\n",
    "\n",
    "1. **Data Preprocessing**: Both RNA and ADT data are normalized, scaled, and processed to create neighbor graphs\n",
    "2. **GAT Training**: Separate GAT models are trained on RNA and ADT data for node classification\n",
    "3. **Embedding Extraction**: Intermediate embeddings are extracted from the trained GAT models\n",
    "4. **VAE Mapping**: A variational autoencoder learns to map RNA embeddings to ADT embeddings through a latent representation\n",
    "5. **Evaluation**: The mapping quality is assessed using MSE, RÂ², and correlation metrics\n",
    "\n",
    "The VAE approach offers several advantages over the MLP:\n",
    "- Provides a probabilistic mapping through the latent space\n",
    "- Can generate novel samples by sampling from the latent distribution\n",
    "- May better capture the underlying data distribution through its regularized latent space\n",
    "- Potentially more robust to noise and outliers\n",
    "\n",
    "The trained models can be used to predict ADT embeddings from RNA data, enabling cross-modal analysis and integration."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
