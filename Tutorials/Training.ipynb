{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Setup Information ===\n",
      "Current directory: /projects/vanaja_lab/satya/DeepOMAPNet/Tutorials\n",
      "Project root: /projects/vanaja_lab/satya/DeepOMAPNet\n",
      "Python version: 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]\n",
      "PyTorch version: 2.9.0+cu128\n",
      "CUDA available: True\n",
      "CUDA device: NVIDIA H200\n",
      "CUDA memory: 150.1 GB\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project root to Python path\n",
    "current_dir = os.getcwd()\n",
    "project_root = os.path.dirname(current_dir)\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "print(\"=== Setup Information ===\")\n",
    "print(f\"Current directory: {current_dir}\")\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added to Python path:\n",
      "- Current directory: /projects/vanaja_lab/satya/DeepOMAPNet/Tutorials\n",
      "- Project root: /projects/vanaja_lab/satya/DeepOMAPNet\n",
      "- Scripts directory exists: True\n",
      "- Scripts/data_provider exists: True\n",
      "Module imports successful!\n"
     ]
    }
   ],
   "source": [
    "import sys, os, importlib\n",
    "\n",
    "# --- Paths ---\n",
    "current_dir = os.getcwd()  # This will be .../DeepOMAPNet/Notebooks\n",
    "project_root = os.path.dirname(current_dir)  # This will be .../DeepOMAPNet\n",
    "\n",
    "# Add project root to Python path\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "print(\"Added to Python path:\")\n",
    "print(f\"- Current directory: {current_dir}\")\n",
    "print(f\"- Project root: {project_root}\")\n",
    "print(f\"- Scripts directory exists: {os.path.exists(os.path.join(project_root, 'scripts'))}\")\n",
    "print(f\"- Scripts/data_provider exists: {os.path.exists(os.path.join(project_root, 'scripts', 'data_provider'))}\")\n",
    "\n",
    "# Clear any cached imports\n",
    "importlib.invalidate_caches()\n",
    "\n",
    "# --- Import modules (module-style, not from ... import ...) ---\n",
    "import scripts.data_provider.data_preprocessing as data_preprocessing\n",
    "import scripts.data_provider.graph_data_builder as graph_data_builder\n",
    "import scripts.model.doNET as doNET\n",
    "import scripts.trainer.gat_trainer as gat_trainer\n",
    "\n",
    "print(\"Module imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All sample IDs in gene data: ['AML0612' 'AML3762' 'AML3133' 'AML2910' 'AML3050' 'AML2451' 'AML056'\n",
      " 'AML073' 'AML055' 'AML048' 'AML052' 'AML2123' 'AML1371' 'AML4340'\n",
      " 'AML4897' 'AML051' 'AML0693' 'AML3948' 'AML3730' 'AML0160' 'AML0310'\n",
      " 'AML0361' 'AML038' 'AML008' 'AML043' 'AML028' 'AML006' 'AML025' 'AML003'\n",
      " 'AML012' 'AML005' 'AML0048' 'AML022' 'AML0024' 'AML009' 'AML026' 'AML001'\n",
      " 'AML0114' 'Control4' 'Control2' 'Control1' 'Control3' 'Control5'\n",
      " 'Control0004' 'Control0058' 'Control0082' 'Control4003' 'Control0005']\n",
      "AML 80% train: ['AML0024', 'AML001', 'AML3050', 'AML4340', 'AML005', 'AML006', 'AML056', 'AML025', 'AML043', 'AML051', 'AML3948', 'AML055', 'AML0693', 'AML1371', 'AML0160', 'AML048', 'AML022', 'AML0612', 'AML028', 'AML2451', 'AML2123', 'AML3762', 'AML0114', 'AML0361', 'AML3133', 'AML012', 'AML026', 'AML2910', 'AML009', 'AML008', 'AML0048']\n",
      "AML 20% test: ['AML052', 'AML038', 'AML3730', 'AML0310', 'AML073', 'AML4897', 'AML003']\n",
      "Control 80% train: ['Control4003', 'Control1', 'Control0004', 'Control0082', 'Control3', 'Control2', 'Control0005', 'Control4']\n",
      "Control 20% test: ['Control5', 'Control0058']\n",
      "Train cells: 158179 | Test cells: 46922\n",
      "\n",
      "============================================================\n",
      "NORMALIZING PROTEIN (ADT) DATA\n",
      "============================================================\n",
      "\n",
      "Step 1: Applying CLR normalization to training protein data...\n",
      "  CLR normalization complete. Shape: (158179, 279)\n",
      "\n",
      "Step 2: Applying z-score normalization to training protein data...\n",
      "  Z-score normalization complete.\n",
      "  Mean of feature means: 0.0000\n",
      "  Mean of feature stds: 0.5178\n",
      "\n",
      "Step 3: Applying CLR normalization to test protein data...\n",
      "  CLR normalization complete. Shape: (46922, 279)\n",
      "\n",
      "Step 4: Applying z-score normalization to test protein data (using training statistics)...\n",
      "  Z-score normalization complete.\n",
      "  Test data normalized using training statistics for consistency.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import scanpy as sc\n",
    "import anndata\n",
    "\n",
    "\n",
    "# Load the preprocessed data\n",
    "from scripts.data_provider.data_preprocessing import prepare_train_test_anndata\n",
    "data = prepare_train_test_anndata()\n",
    "rna_adata = data[0]  # RNA data\n",
    "rna_test = data[1]\n",
    "adt_adata = data[2]   # ADT data\n",
    "adt_test = data[3]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rna_adata = sc.read_h5ad(\"/projects/vanaja_lab/satya/DeepOMAPNet/GSE116256.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AML samples: 118224\n",
      "Normal samples: 39955\n"
     ]
    }
   ],
   "source": [
    "# One-liner using pandas string methods\n",
    "rna_adata.obs['aml_labels'] = rna_adata.obs['samples'].str.startswith('AML').astype(int)\n",
    "\n",
    "# Convert to numpy array for training\n",
    "aml_labels_array = rna_adata.obs['aml_labels'].values\n",
    "\n",
    "# Check distribution\n",
    "print(f\"AML samples: {aml_labels_array.sum()}\")\n",
    "print(f\"Normal samples: {len(aml_labels_array) - aml_labels_array.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Converting to PyTorch Geometric Format ===\n",
      "Converting RNA data...\n",
      "build_pyg_data called with use_pca=True\n",
      "Input adata shape: (158179, 2000)\n",
      "Available obsm keys: ['X_integrated.cca', 'X_pca', 'X_scVI', 'X_umap', 'X_umap.unintegrated']\n",
      "Computing PCA with exactly 50 components...\n",
      "PCA computed, shape: (158179, 50)\n",
      "Computing neighbor graph first...\n",
      "Computing leiden clusters first...\n",
      "Using PCA features, shape: (158179, 50)\n",
      "RNA PyG data: Data(x=[158179, 50], edge_index=[2, 1916900], y=[158179])\n",
      "Converting ADT data...\n",
      "build_pyg_data called with use_pca=True\n",
      "Input adata shape: (158179, 279)\n",
      "Available obsm keys: []\n",
      "Computing PCA with exactly 50 components...\n",
      "PCA computed, shape: (158179, 50)\n",
      "Computing neighbor graph first...\n",
      "Computing leiden clusters first...\n",
      "Using PCA features, shape: (158179, 50)\n",
      "ADT PyG data: Data(x=[158179, 50], edge_index=[2, 1706010], y=[158179])\n",
      "✅ RNA and ADT data have same number of nodes\n",
      "✅ PyTorch Geometric conversion complete!\n"
     ]
    }
   ],
   "source": [
    "# Convert AnnData to PyTorch Geometric format\n",
    "print(\"=== Converting to PyTorch Geometric Format ===\")\n",
    "from scripts.data_provider.graph_data_builder import build_pyg_data\n",
    "# Convert RNA data\n",
    "print(\"Converting RNA data...\")\n",
    "rna_pyg_data = build_pyg_data(rna_adata)\n",
    "print(f\"RNA PyG data: {rna_pyg_data}\")\n",
    "\n",
    "# Convert ADT data\n",
    "print(\"Converting ADT data...\")\n",
    "adt_pyg_data = build_pyg_data(adt_adata)\n",
    "print(f\"ADT PyG data: {adt_pyg_data}\")\n",
    "\n",
    "\n",
    "if rna_pyg_data.num_nodes != adt_pyg_data.num_nodes:\n",
    "    print(\"⚠️  Warning: RNA and ADT data have different number of nodes!\")\n",
    "else:\n",
    "    print(\"✅ RNA and ADT data have same number of nodes\")\n",
    "\n",
    "print(\"✅ PyTorch Geometric conversion complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1) Extract labels from AnnData\n",
    "labels_series = rna_adata.obs['Cell_type_identity'].astype('category')\n",
    "\n",
    "# 2) Map to integer classes\n",
    "celltype_to_idx = {cat: i for i, cat in enumerate(labels_series.cat.categories)}\n",
    "idx_to_celltype = {i: cat for cat, i in celltype_to_idx.items()}\n",
    "celltype_labels = labels_series.cat.codes.to_numpy()  # shape [N], ints in [0, C-1]\n",
    "num_cell_types = len(celltype_to_idx)\n",
    "num_cell_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Training Enhanced GATWithTransformerFusion Model ===\n",
      "Training started at: 2025-11-01 15:25:54.440155\n",
      "GPU memory fraction set to 50%\n",
      "Using device: cuda\n",
      "Data splits — train: 126543, val: 15817, test: 15819\n",
      "Preprocessing RNA data using AnnData...\n",
      "RNA preprocessing applied: torch.Size([158179, 2000])\n",
      "Updated RNA input dimension after preprocessing: 2000\n",
      "Loading ADT data for preprocessing...\n",
      "Using AnnData object for ADT preprocessing...\n",
      "Data from AnnData loaded: torch.Size([158179, 279])\n",
      "Data appears to be already normalized. Using provided statistics.\n",
      "Statistics: mean=-0.0000, std=1.0000\n",
      "AML labels processed: (158179,), Normal: 39955, AML: 118224\n",
      "AML labels split — train: 126543 (Normal: 31893, AML: 94650), val: 15817 (Normal: 4026, AML: 11791), test: 15819 (Normal: 4036, AML: 11783)\n",
      "Updated ADT output dimension after preprocessing: 279\n",
      "Model parameters: 483,503\n",
      "Model moved to cuda\n",
      "RNA data moved to cuda\n",
      "ADT data moved to cuda\n",
      "AML labels moved to cuda: torch.Size([158179])\n",
      "CellType labels moved to cuda: torch.Size([158179])\n",
      "Starting training...\n",
      "Epoch 001 | ADT Loss 1.217625 AML Loss 1.142816 Reg Loss 0.016936 | Val MSE 1.082012 R² -0.0979 | Test MSE 1.091927 R² -0.0972 | Val AML Acc 0.255 F1 0.000 | Test AML Acc 0.255 F1 0.000\n",
      "Epoch 010 | ADT Loss 1.175702 AML Loss 0.731035 Reg Loss 0.016785 | Val MSE 1.050048 R² -0.0655 | Test MSE 1.059677 R² -0.0648 | Val AML Acc 0.745 F1 0.854 | Test AML Acc 0.745 F1 0.854\n",
      "Epoch 020 | ADT Loss 1.117298 AML Loss 0.655457 Reg Loss 0.016547 | Val MSE 1.017422 R² -0.0323 | Test MSE 1.026911 R² -0.0319 | Val AML Acc 0.745 F1 0.854 | Test AML Acc 0.745 F1 0.854\n",
      "Epoch 030 | ADT Loss 1.071115 AML Loss 0.614849 Reg Loss 0.016309 | Val MSE 0.996209 R² -0.0108 | Test MSE 1.005723 R² -0.0106 | Val AML Acc 0.745 F1 0.854 | Test AML Acc 0.745 F1 0.854\n",
      "Epoch 040 | ADT Loss 1.045148 AML Loss 0.582771 Reg Loss 0.016095 | Val MSE 0.988225 R² -0.0027 | Test MSE 0.997764 R² -0.0026 | Val AML Acc 0.745 F1 0.854 | Test AML Acc 0.745 F1 0.854\n",
      "Epoch 050 | ADT Loss 1.033322 AML Loss 0.540493 Reg Loss 0.015919 | Val MSE 0.984513 R² 0.0010 | Test MSE 0.994066 R² 0.0011 | Val AML Acc 0.746 F1 0.855 | Test AML Acc 0.746 F1 0.854\n",
      "Epoch 060 | ADT Loss 1.024225 AML Loss 0.489805 Reg Loss 0.015774 | Val MSE 0.979896 R² 0.0057 | Test MSE 0.989280 R² 0.0059 | Val AML Acc 0.858 F1 0.911 | Test AML Acc 0.862 F1 0.913\n",
      "Epoch 070 | ADT Loss 1.016628 AML Loss 0.445901 Reg Loss 0.015657 | Val MSE 0.974599 R² 0.0111 | Test MSE 0.983830 R² 0.0114 | Val AML Acc 0.901 F1 0.935 | Test AML Acc 0.900 F1 0.934\n",
      "Epoch 080 | ADT Loss 1.009277 AML Loss 0.418304 Reg Loss 0.015564 | Val MSE 0.968956 R² 0.0168 | Test MSE 0.977977 R² 0.0173 | Val AML Acc 0.916 F1 0.945 | Test AML Acc 0.917 F1 0.945\n",
      "Epoch 090 | ADT Loss 1.003116 AML Loss 0.394925 Reg Loss 0.015492 | Val MSE 0.962372 R² 0.0235 | Test MSE 0.971062 R² 0.0242 | Val AML Acc 0.930 F1 0.953 | Test AML Acc 0.930 F1 0.953\n",
      "Epoch 100 | ADT Loss 0.997009 AML Loss 0.381779 Reg Loss 0.015436 | Val MSE 0.955842 R² 0.0301 | Test MSE 0.964297 R² 0.0310 | Val AML Acc 0.937 F1 0.958 | Test AML Acc 0.938 F1 0.958\n",
      "Epoch 110 | ADT Loss 0.993584 AML Loss 0.363780 Reg Loss 0.015392 | Val MSE 0.950327 R² 0.0357 | Test MSE 0.958683 R² 0.0366 | Val AML Acc 0.938 F1 0.959 | Test AML Acc 0.937 F1 0.958\n",
      "Epoch 120 | ADT Loss 0.987310 AML Loss 0.351894 Reg Loss 0.015355 | Val MSE 0.944547 R² 0.0416 | Test MSE 0.952754 R² 0.0426 | Val AML Acc 0.941 F1 0.961 | Test AML Acc 0.942 F1 0.961\n",
      "Epoch 130 | ADT Loss 0.983595 AML Loss 0.343821 Reg Loss 0.015322 | Val MSE 0.940906 R² 0.0453 | Test MSE 0.949097 R² 0.0463 | Val AML Acc 0.935 F1 0.957 | Test AML Acc 0.934 F1 0.957\n",
      "Epoch 140 | ADT Loss 0.980663 AML Loss 0.336904 Reg Loss 0.015292 | Val MSE 0.937133 R² 0.0491 | Test MSE 0.945215 R² 0.0502 | Val AML Acc 0.931 F1 0.955 | Test AML Acc 0.931 F1 0.955\n",
      "Epoch 150 | ADT Loss 0.978140 AML Loss 0.330620 Reg Loss 0.015267 | Val MSE 0.934341 R² 0.0520 | Test MSE 0.942287 R² 0.0531 | Val AML Acc 0.929 F1 0.954 | Test AML Acc 0.929 F1 0.954\n",
      "Epoch 160 | ADT Loss 0.975557 AML Loss 0.320635 Reg Loss 0.015244 | Val MSE 0.929752 R² 0.0566 | Test MSE 0.937633 R² 0.0578 | Val AML Acc 0.936 F1 0.958 | Test AML Acc 0.936 F1 0.958\n",
      "Epoch 170 | ADT Loss 0.969810 AML Loss 0.312333 Reg Loss 0.015227 | Val MSE 0.926992 R² 0.0594 | Test MSE 0.934792 R² 0.0607 | Val AML Acc 0.932 F1 0.955 | Test AML Acc 0.934 F1 0.957\n",
      "Epoch 180 | ADT Loss 0.968294 AML Loss 0.309550 Reg Loss 0.015218 | Val MSE 0.921381 R² 0.0651 | Test MSE 0.929030 R² 0.0664 | Val AML Acc 0.949 F1 0.966 | Test AML Acc 0.949 F1 0.966\n",
      "Epoch 190 | ADT Loss 0.962715 AML Loss 0.301101 Reg Loss 0.015222 | Val MSE 0.918006 R² 0.0685 | Test MSE 0.925603 R² 0.0699 | Val AML Acc 0.949 F1 0.966 | Test AML Acc 0.950 F1 0.967\n",
      "Epoch 200 | ADT Loss 0.956540 AML Loss 0.291293 Reg Loss 0.015243 | Val MSE 0.909850 R² 0.0768 | Test MSE 0.917180 R² 0.0784 | Val AML Acc 0.956 F1 0.970 | Test AML Acc 0.956 F1 0.970\n",
      "Epoch 210 | ADT Loss 0.954100 AML Loss 0.289383 Reg Loss 0.015281 | Val MSE 0.907127 R² 0.0796 | Test MSE 0.914379 R² 0.0812 | Val AML Acc 0.950 F1 0.967 | Test AML Acc 0.951 F1 0.968\n",
      "Epoch 220 | ADT Loss 0.948759 AML Loss 0.280682 Reg Loss 0.015334 | Val MSE 0.895723 R² 0.0911 | Test MSE 0.902332 R² 0.0933 | Val AML Acc 0.958 F1 0.972 | Test AML Acc 0.959 F1 0.973\n",
      "Epoch 230 | ADT Loss 0.940999 AML Loss 0.277886 Reg Loss 0.015404 | Val MSE 0.887444 R² 0.0995 | Test MSE 0.894057 R² 0.1016 | Val AML Acc 0.948 F1 0.965 | Test AML Acc 0.948 F1 0.965\n",
      "Epoch 240 | ADT Loss 0.927506 AML Loss 0.271444 Reg Loss 0.015488 | Val MSE 0.857831 R² 0.1296 | Test MSE 0.863365 R² 0.1324 | Val AML Acc 0.957 F1 0.972 | Test AML Acc 0.958 F1 0.972\n",
      "Epoch 250 | ADT Loss 0.899478 AML Loss 0.266671 Reg Loss 0.015594 | Val MSE 0.816389 R² 0.1716 | Test MSE 0.821007 R² 0.1750 | Val AML Acc 0.959 F1 0.972 | Test AML Acc 0.959 F1 0.972\n",
      "Epoch 260 | ADT Loss 0.873433 AML Loss 0.268082 Reg Loss 0.015715 | Val MSE 0.784810 R² 0.2037 | Test MSE 0.788678 R² 0.2075 | Val AML Acc 0.941 F1 0.961 | Test AML Acc 0.942 F1 0.962\n",
      "Epoch 270 | ADT Loss 0.857578 AML Loss 0.260705 Reg Loss 0.015834 | Val MSE 0.754222 R² 0.2347 | Test MSE 0.758329 R² 0.2380 | Val AML Acc 0.957 F1 0.971 | Test AML Acc 0.956 F1 0.971\n",
      "Epoch 280 | ADT Loss 0.841159 AML Loss 0.262604 Reg Loss 0.015958 | Val MSE 0.726219 R² 0.2631 | Test MSE 0.730040 R² 0.2664 | Val AML Acc 0.960 F1 0.973 | Test AML Acc 0.959 F1 0.973\n",
      "Epoch 290 | ADT Loss 0.819737 AML Loss 0.263695 Reg Loss 0.016086 | Val MSE 0.706478 R² 0.2832 | Test MSE 0.709993 R² 0.2865 | Val AML Acc 0.959 F1 0.973 | Test AML Acc 0.959 F1 0.973\n",
      "Epoch 300 | ADT Loss 0.811237 AML Loss 0.253730 Reg Loss 0.016210 | Val MSE 0.692981 R² 0.2969 | Test MSE 0.696399 R² 0.3002 | Val AML Acc 0.960 F1 0.973 | Test AML Acc 0.960 F1 0.973\n",
      "Epoch 310 | ADT Loss 0.793457 AML Loss 0.249615 Reg Loss 0.016332 | Val MSE 0.671213 R² 0.3189 | Test MSE 0.674921 R² 0.3218 | Val AML Acc 0.959 F1 0.973 | Test AML Acc 0.960 F1 0.973\n",
      "Epoch 320 | ADT Loss 0.781631 AML Loss 0.239405 Reg Loss 0.016459 | Val MSE 0.660862 R² 0.3294 | Test MSE 0.664652 R² 0.3321 | Val AML Acc 0.960 F1 0.973 | Test AML Acc 0.960 F1 0.973\n",
      "Epoch 330 | ADT Loss 0.771942 AML Loss 0.243579 Reg Loss 0.016598 | Val MSE 0.658482 R² 0.3319 | Test MSE 0.661958 R² 0.3348 | Val AML Acc 0.963 F1 0.975 | Test AML Acc 0.964 F1 0.976\n",
      "Epoch 340 | ADT Loss 0.761515 AML Loss 0.230361 Reg Loss 0.016726 | Val MSE 0.640198 R² 0.3504 | Test MSE 0.644527 R² 0.3523 | Val AML Acc 0.960 F1 0.973 | Test AML Acc 0.959 F1 0.973\n",
      "Epoch 350 | ADT Loss 0.754234 AML Loss 0.227496 Reg Loss 0.016852 | Val MSE 0.632967 R² 0.3577 | Test MSE 0.637559 R² 0.3593 | Val AML Acc 0.965 F1 0.976 | Test AML Acc 0.965 F1 0.976\n",
      "Epoch 360 | ADT Loss 0.741006 AML Loss 0.222808 Reg Loss 0.016975 | Val MSE 0.618143 R² 0.3728 | Test MSE 0.622547 R² 0.3744 | Val AML Acc 0.964 F1 0.976 | Test AML Acc 0.965 F1 0.976\n",
      "Epoch 370 | ADT Loss 0.733086 AML Loss 0.226382 Reg Loss 0.017093 | Val MSE 0.624704 R² 0.3661 | Test MSE 0.628953 R² 0.3680 | Val AML Acc 0.963 F1 0.975 | Test AML Acc 0.962 F1 0.975\n",
      "Epoch 380 | ADT Loss 0.727330 AML Loss 0.218213 Reg Loss 0.017211 | Val MSE 0.605774 R² 0.3853 | Test MSE 0.610448 R² 0.3866 | Val AML Acc 0.954 F1 0.970 | Test AML Acc 0.952 F1 0.969\n",
      "Epoch 390 | ADT Loss 0.722603 AML Loss 0.218143 Reg Loss 0.017333 | Val MSE 0.592420 R² 0.3989 | Test MSE 0.597803 R² 0.3993 | Val AML Acc 0.957 F1 0.971 | Test AML Acc 0.956 F1 0.971\n",
      "Epoch 400 | ADT Loss 0.715497 AML Loss 0.212681 Reg Loss 0.017448 | Val MSE 0.615226 R² 0.3757 | Test MSE 0.619107 R² 0.3779 | Val AML Acc 0.967 F1 0.978 | Test AML Acc 0.967 F1 0.978\n",
      "Epoch 410 | ADT Loss 0.709556 AML Loss 0.207141 Reg Loss 0.017560 | Val MSE 0.568911 R² 0.4227 | Test MSE 0.573665 R² 0.4235 | Val AML Acc 0.969 F1 0.979 | Test AML Acc 0.970 F1 0.980\n",
      "Epoch 420 | ADT Loss 0.703985 AML Loss 0.211454 Reg Loss 0.017681 | Val MSE 0.563511 R² 0.4282 | Test MSE 0.568311 R² 0.4289 | Val AML Acc 0.961 F1 0.974 | Test AML Acc 0.958 F1 0.972\n",
      "Epoch 430 | ADT Loss 0.697749 AML Loss 0.205611 Reg Loss 0.017797 | Val MSE 0.600199 R² 0.3910 | Test MSE 0.603972 R² 0.3931 | Val AML Acc 0.936 F1 0.958 | Test AML Acc 0.931 F1 0.955\n",
      "Epoch 440 | ADT Loss 0.695696 AML Loss 0.205788 Reg Loss 0.017906 | Val MSE 0.558477 R² 0.4333 | Test MSE 0.562937 R² 0.4343 | Val AML Acc 0.951 F1 0.968 | Test AML Acc 0.948 F1 0.966\n",
      "Epoch 450 | ADT Loss 0.689079 AML Loss 0.206228 Reg Loss 0.018001 | Val MSE 0.544820 R² 0.4472 | Test MSE 0.549659 R² 0.4477 | Val AML Acc 0.963 F1 0.975 | Test AML Acc 0.960 F1 0.973\n",
      "Epoch 460 | ADT Loss 0.681231 AML Loss 0.202029 Reg Loss 0.018099 | Val MSE 0.552616 R² 0.4393 | Test MSE 0.557212 R² 0.4401 | Val AML Acc 0.947 F1 0.965 | Test AML Acc 0.943 F1 0.963\n",
      "Epoch 470 | ADT Loss 0.680671 AML Loss 0.208122 Reg Loss 0.018193 | Val MSE 0.590362 R² 0.4010 | Test MSE 0.593529 R² 0.4036 | Val AML Acc 0.960 F1 0.973 | Test AML Acc 0.958 F1 0.972\n",
      "Epoch 480 | ADT Loss 0.679311 AML Loss 0.208425 Reg Loss 0.018283 | Val MSE 0.600074 R² 0.3911 | Test MSE 0.603084 R² 0.3940 | Val AML Acc 0.959 F1 0.973 | Test AML Acc 0.957 F1 0.972\n",
      "Epoch 490 | ADT Loss 0.675426 AML Loss 0.202564 Reg Loss 0.018367 | Val MSE 0.530941 R² 0.4613 | Test MSE 0.535658 R² 0.4617 | Val AML Acc 0.951 F1 0.968 | Test AML Acc 0.947 F1 0.965\n",
      "Epoch 500 | ADT Loss 0.666526 AML Loss 0.201261 Reg Loss 0.018457 | Val MSE 0.518007 R² 0.4744 | Test MSE 0.522192 R² 0.4753 | Val AML Acc 0.970 F1 0.980 | Test AML Acc 0.969 F1 0.979\n",
      "Epoch 510 | ADT Loss 0.666159 AML Loss 0.197887 Reg Loss 0.018557 | Val MSE 0.545959 R² 0.4460 | Test MSE 0.549125 R² 0.4482 | Val AML Acc 0.958 F1 0.972 | Test AML Acc 0.956 F1 0.971\n",
      "Epoch 520 | ADT Loss 0.664077 AML Loss 0.200101 Reg Loss 0.018667 | Val MSE 0.542899 R² 0.4491 | Test MSE 0.546495 R² 0.4508 | Val AML Acc 0.952 F1 0.968 | Test AML Acc 0.948 F1 0.966\n",
      "Epoch 530 | ADT Loss 0.661809 AML Loss 0.197983 Reg Loss 0.018784 | Val MSE 0.513298 R² 0.4792 | Test MSE 0.517335 R² 0.4801 | Val AML Acc 0.957 F1 0.972 | Test AML Acc 0.952 F1 0.969\n",
      "Epoch 540 | ADT Loss 0.659626 AML Loss 0.195794 Reg Loss 0.018883 | Val MSE 0.519380 R² 0.4730 | Test MSE 0.523207 R² 0.4742 | Val AML Acc 0.963 F1 0.975 | Test AML Acc 0.959 F1 0.973\n",
      "Epoch 550 | ADT Loss 0.653939 AML Loss 0.195660 Reg Loss 0.018976 | Val MSE 0.527388 R² 0.4649 | Test MSE 0.530907 R² 0.4665 | Val AML Acc 0.946 F1 0.965 | Test AML Acc 0.943 F1 0.963\n",
      "Loaded best model (val R²=0.4792).\n",
      "\n",
      "Final metrics:\n",
      "  Train | MSE 0.524273  RMSE 0.724067  MAE 0.464314  R² 0.4770  r_mean 0.691  ρ_mean 0.715\n",
      "         | AML Acc 0.954  Precision 0.954  Recall 0.986  F1 0.970  AUC 0.991\n",
      "  Val   | MSE 0.513298  RMSE 0.716448  MAE 0.460285  R² 0.4792  r_mean 0.693  ρ_mean 0.716\n",
      "         | AML Acc 0.957  Precision 0.958  Recall 0.986  F1 0.972  AUC 0.992\n",
      "  Test  | MSE 0.517335  RMSE 0.719260  MAE 0.461195  R² 0.4801  r_mean 0.693  ρ_mean 0.716\n",
      "         | AML Acc 0.952  Precision 0.954  Recall 0.984  F1 0.969  AUC 0.992\n",
      "============================================================\n",
      "\n",
      "=== Training Complete ===\n",
      "Training finished at: 2025-11-01 15:30:21.284153\n",
      "Total training time: 0:04:26.843998\n",
      "Training time per epoch: 0:00:00.485171\n",
      "\n",
      "=== Enhanced Training Results ===\n",
      "Final training loss: 0.653939\n",
      "Final validation MSE: 0.527388\n",
      "Final validation R²: 0.4649\n",
      "Final test MSE: 0.530907\n",
      "Final test R²: 0.4665\n",
      "Final regularization loss: 0.018976\n",
      "Average regularization loss: 0.016597\n",
      "\n",
      "=== Testing Enhanced Model Capabilities ===\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import importlib\n",
    "import time\n",
    "from datetime import datetime\n",
    "from scripts.trainer import gat_trainer\n",
    "\n",
    "# Reload the module\n",
    "importlib.reload(gat_trainer)\n",
    "\n",
    "# Re-import the functions you need\n",
    "from scripts.trainer.gat_trainer import train_gat_transformer_fusion\n",
    "\n",
    "print(\"=== Training Enhanced GATWithTransformerFusion Model ===\")\n",
    "\n",
    "# Training parameters (only parameters accepted by train_gat_transformer_fusion)\n",
    "training_config = {\n",
    "    'epochs': 550,\n",
    "    'use_cpu_fallback': False,\n",
    "    'seed': 42,\n",
    "    'learning_rate': 1e-3,           # Changed from 'lr'\n",
    "    'weight_decay': 1e-4,\n",
    "    'dropout_rate': 0.6,             # Changed from 'dropout'\n",
    "    'hidden_channels': 64,\n",
    "    \n",
    "    'num_heads': 4,                  # Changed from 'heads'\n",
    "    'num_attention_heads': 4,        # Changed from 'nhead'\n",
    "    'num_layers': 2,\n",
    "    'use_mixed_precision': True,     # Changed from 'amp'\n",
    "    'early_stopping_patience': 5,    # Changed from 'patience'\n",
    "    'num_cell_types': num_cell_types\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "start_time = datetime.now()\n",
    "print(f\"Training started at: {start_time}\")\n",
    "\n",
    "\n",
    "# Train the enhanced model\n",
    "trained_model, rna_data_with_masks, adt_data_with_masks, training_history,adt_mean, adt_std, node_degrees_rna, node_degrees_adt, clustering_coeffs_rna, clustering_coeffs_adt = train_gat_transformer_fusion(\n",
    "    rna_data=rna_pyg_data,\n",
    "    adt_data=adt_pyg_data,\n",
    "    aml_labels=aml_labels_array,\n",
    "    rna_anndata = rna_adata,\n",
    "    adt_anndata = adt_adata,\n",
    "    celltype_labels=celltype_labels, \n",
    "    celltype_weight=1.0,\n",
    "    **training_config\n",
    ")\n",
    "\n",
    "end_time = datetime.now()\n",
    "training_duration = end_time - start_time\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n=== Training Complete ===\")\n",
    "print(f\"Training finished at: {end_time}\")\n",
    "print(f\"Total training time: {training_duration}\")\n",
    "print(f\"Training time per epoch: {training_duration / training_config['epochs']}\")\n",
    "\n",
    "# Enhanced training results analysis\n",
    "print(f\"\\n=== Enhanced Training Results ===\")\n",
    "print(f\"Final training loss: {training_history['train_loss'][-1]:.6f}\")\n",
    "print(f\"Final validation MSE: {training_history['val_MSE'][-1]:.6f}\")\n",
    "print(f\"Final validation R²: {training_history['val_R2'][-1]:.4f}\")\n",
    "print(f\"Final test MSE: {training_history['test_MSE'][-1]:.6f}\")\n",
    "print(f\"Final test R²: {training_history['test_R2'][-1]:.4f}\")\n",
    "\n",
    "# Regularization loss analysis\n",
    "if 'reg_loss' in training_history:\n",
    "    print(f\"Final regularization loss: {training_history['reg_loss'][-1]:.6f}\")\n",
    "    print(f\"Average regularization loss: {sum(training_history['reg_loss']) / len(training_history['reg_loss']):.6f}\")\n",
    "\n",
    "# Model capabilities test\n",
    "print(f\"\\n=== Testing Enhanced Model Capabilities ===\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model = trained_model\n",
    "\n",
    "# Option 2: Save only the weights\n",
    "torch.save(model.state_dict(), \"DeepOMAPNet_weights.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scVI",
   "language": "python",
   "name": "scvi-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
